<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: methodology | Infonomics Blog]]></title>
  <link href="http://infonomics.ltd.uk/news/blog/categories/methodology/atom.xml" rel="self"/>
  <link href="http://infonomics.ltd.uk/news/"/>
  <updated>2014-01-20T09:58:47+01:00</updated>
  <id>http://infonomics.ltd.uk/news/</id>
  <author>
    <name><![CDATA[Robin Gower]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to choose a Sample Size]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/10/18/how-to-choose-a-sample-size/"/>
    <updated>2013-10-18T09:26:00+02:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/10/18/how-to-choose-a-sample-size</id>
    <content type="html"><![CDATA[<p>I&rsquo;m often asked to provide advice on this question. The short answer is usually &ldquo;it depends&rdquo;. This is the longer answer that explains what it depends on!</p>

<p>The size of a sample doesn&rsquo;t need to be as large people usually think. The sample size doesn&rsquo;t actually depend upon the population size (i.e. the things &ndash; individuals, organisations, or projects &ndash; that the sample is supposed to represent). Indeed National opinion polls usually include no more than 1,000 responses (although they do require an <a href="http://en.wikipedia.org/wiki/Sampling_bias#Historical_examples">unbiased sample</a>).</p>

<p>So what does sample size depend upon..?</p>

<!-- more -->


<h2>Why choosing a sample size is important</h2>

<p>A sample is a subset of a population that you wish to study. A sample is typically chosen for research because it is too difficult or expensive to talk to the whole population. Fortunately statistical theory demonstrates that it is possible for responses taken from a sample to represent a wider population.</p>

<p>This post explains some of things to consider when choosing a representative sample.</p>

<h2>Total sample size</h2>

<p>The choice of overall sample size is driven by three statistical considerations</p>

<ul>
<li>what effect size you want to capture (i.e. the expected difference between the things you&rsquo;re comparing),</li>
<li>how willing you are to miss effects that are present, and</li>
<li>how willing you are to mistakenly identify effects that aren&rsquo;t present.</li>
</ul>


<p>We can set the latter two according to statistical convention (significance level of 95%, and a statistical power of 80%). Effect size really depends upon the context.</p>

<p>The main unknown here will be the standard deviation &ndash; that is to say the extent to which the things you&rsquo;re measuring vary from case-to-case. This, of course, determines the degree to which we can draw conclusions about the whole population by looking at a sample.</p>

<p>We define the effect size relative to the standard deviation. The larger the effect size, the smaller the sample needed to detect it. Drawing on social research, Cohen suggests that 20%, 50%, and 80% (of the standard deviation) represent small, medium, and large effect sizes respectively.</p>

<h2>Representativeness of sub-groups</h2>

<p>There are a couple of issues here. First, the overall results should be representative (you don&rsquo;t want to miss-out certain groups) and unbiased (if the sample structure doesn&rsquo;t match the proportion of the overall population the results will need to be weighted). Second, you might want to compare the results of the sub-groups (e.g. treatment vs control groups, cohorts by gender/ age/ ethncitiy, or to see if results change over time/ by location).</p>

<p>Bear in mind the above discussion on sample sizes. The rule of thumb value for sub-samples is to have 30 responses &ndash; is consistent with an effect size of 73%. This would, of course, be exhaustive for some of the smaller sub-groups. You may need to combine sub-groups or deal with this variability in the analysis rather than in the sample design &ndash; e.g. removing outliers after the responses have been gathered). You might aim for 30 responses among the larger groups but relax this for smaller ones.</p>

<h2>Response rates</h2>

<p>This depends on how and who contacts the projects. Fortunately, as a funder you&rsquo;re likely to have their attention! In my experience of contacting beneficiaries on behalf of public/ social projects, a 30% response rate is good going. Don&rsquo;t be surprised if you receive a response rate lower than 1% for an email survey (although the ease of contacting a wide range should mean this is still an effective method). I&rsquo;d aim to contact about 5 times as many people as you need responses.</p>

<p>The overall decision on sample size ought to take into account the cost and difficulty of acquiring responses and the observed variability of responses. You might want to bootstrap the process by running an initial small sample that would serve to identify whether further booster samples are required (e.g. &ldquo;the overall result is satisfactory but we&rsquo;re concerned about the apparent discrepancy among very large projects so they&rsquo;ll be the subject of further research&rdquo;). Bear in mind that this is by no means a precise science and this guidance should be considered in the context of practical concerns. Perhaps the most useful contribution of statistical advice &ndash; rather than identifying a magic number &ndash; is to help you to understand the overall patterns (variability matters, diminishing returns apply, very large/ small projects might need to be considered separately).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[9 Tools Every Information Analyst Should Have]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/01/30/9-tools-every-information-analyst-should-have/"/>
    <updated>2013-01-30T12:15:00+01:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/01/30/9-tools-every-information-analyst-should-have</id>
    <content type="html"><![CDATA[<p>I was interested to read Rebecca Murphey&rsquo;s <a href="http://rmurphey.com/blog/2012/04/12/a-baseline-for-front-end-developers/">Baseline for Front End Developers</a>  in which the author realizes that she has begun to take for granted a basic set of tools of the trade. She presents &ldquo;a few things that [she] wants to start expecting people to be familiar with, along with some resources you can use if you feel like you need to get up to speed&rdquo;. I&rsquo;d like to pay homage to that article and revise it in light of <em>the tools I believe are essential for an Information Analyst</em>. There is considerable overlap in the tools suggested although the application will differ.</p>

<!--more -->


<p><img class="<a" src="href="http://imgs.xkcd.com/comics/regular_expressions.png">http://imgs.xkcd.com/comics/regular_expressions.png</a>" title="&lsquo;Wait, forgot to escape a space.  Wheeeeee[taptaptap]eeeeee.&rsquo; &lsquo;XKCD Regular Expressions Comic&rsquo;" ></p>

<h2>GNU/ UNIX Command Line Tools</h2>

<p>These tools are especially powerful because they don&rsquo;t require you to load the entire file into memory. This makes it possible to work with very large files efficiently.</p>

<ul>
<li><code>head</code>, <code>tail</code>, <code>less</code>, <code>cat</code> etc for looking at files</li>
<li><code>awk</code>, <code>sed</code>, and <code>tr</code> for processing and altering files</li>
<li><code>cut</code>, <code>paste</code>, and <code>join</code> again for editing and combining text files</li>
<li><code>sort</code> and <code>uniq</code></li>
<li><code>wc</code> for counting lines/ words/ characters</li>
<li><code>iconv</code> for converting between different encodings</li>
</ul>


<p>Some useful resources include:</p>

<ul>
<li><a href="http://blog.sanctum.geek.nz/series/unix-as-ide/">Unix as an IDE</a></li>
<li><a href="http://www.ibm.com/developerworks/linux/library/l-textutils/index.html">Simplify data extraction using Linux text utilities</a></li>
<li><a href="http://tldp.org/LDP/abs/html/textproc.html">Text Processing Commands in the Advanced Bash-Scripting Guide</a></li>
<li><a href="http://funarg.nfshost.com/r2/notes/sed-return-comma.html">Replacing returns with commas in Sed</a></li>
<li><a href="https://github.com/robbyrussell/oh-my-zsh">Oh my zsh</a> &ndash; zshell a replacement for bash</li>
</ul>


<h2>Regular Expressions</h2>

<p>Regexs are extremely powerful, if sometimes counter-intuitive. They are vital for defining patterns and replacements in text processing. Although some <a href="http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454">debate</a> exists a to whether they should be used to parse structured formats like XHTML, they are certainly a vital tool.</p>

<p>Having said which, I find this quote pretty amusing!
<blockquote><p>Some people, when confronted with a problem, think “I know, I&rsquo;ll use regular expressions.” Now they have two problems.</p><footer><strong>Jamie Zawinski <a href="http://regex.info/blog/2006-09-15/247">http://regex.info/blog/2006-09-15/247</a> Jeffrey Friedl&rsquo;s Blog</strong></footer></blockquote></p>

<ul>
<li><a href="http://gskinner.com/RegExr/">RegExr</a> &ndash; Regex building and testing tool</li>
<li><a href="http://www.regexper.com/">Regexper</a> &ndash; visualisation tool</li>
<li><a href="http://www.regular-expressions.info/">Reference Docs</a></li>
<li>An example for <a href="http://www.softwareprojects.com/resources/programming/t-validate-a-credit-card-1682.html">validating credit card numbers</a> or <a href="http://stackoverflow.com/a/164994/187009">validating UK Postcodes</a></li>
</ul>


<h2>A Programming/ Scripting Language</h2>

<p>If nothing else, these provide the glue to connect different sections of your data pipeline. The incredible range of libraries avavailable to most scripting languages will make quick work of many common information processing tasks: parsing, spidering, <abbr title="Extraction, Translation, and Loading">ETL</abbr> etc. The choice of language is down to personal preference or compatibility with the team.
It should suffice to say that knowledge of a scripting language is necessary and I don&rsquo;t feel the need to recommend a particular language. Having said which, I can&rsquo;t resist saying that I prefer <code>ruby</code> but have dabbled in <code>perl</code> and <code>python</code>. I find the following ruby gems indispensible: <code>mechanize</code>, <code>nokogiri</code>, <code>fastercsv</code> (now <code>CSV</code> in the 1.9 standard library), and <code>json</code>. I&rsquo;ve also been using <code>javascript</code> and node for scripting outside of the browser as asynchronous processing has proved to be very quick (if a little peculiar to code at first).</p>

<ul>
<li><a href="http://mislav.uniqpath.com/poignant-guide/book/chapter-1.html">Why The Lucky Stiff&rsquo;s (poignant) Guide to Ruby</a></li>
<li><a href="http://www.rubyist.net/~slagell/ruby/">A guide written by Matz (the original author of Ruby)</a></li>
<li><a href="https://github.com/styleguide/ruby">Github&rsquo;s Ruby Style Guide</a></li>
<li><a href="https://www.ruby-toolbox.com/">Ruby Toolbox</a> &ndash; for finding and selecting gems</li>
<li>John Ressig&rsquo;s <a href="http://ejohn.org/apps/learn/">Learning Advanced Javascript</a></li>
</ul>


<h2>Text data files</h2>

<p>Probably the most basic technology used in information analysis, the flat file is the lowest common denominator. You can almost guarantee that everyone will be able to cope with <code>csv</code> files (even if they need to be translated before loading into another tool). The expressiveness and rigour of <code>XML</code> makes it another vital tool although it&rsquo;s rapidly being superceded in bandwidth-conscious realm of http by the less verbose <code>json</code>.
Information analysts ought to be able to handle (read, understand, parse and translate) files in these all of formats even if they immediately convert them to a prefered type.</p>

<ul>
<li><a href="http://www.delicious.com/redirect?url=http%3A//andrewjwelch.com/code/xslt/csv/csv-to-xml_v2.html">XSLT for CSV 2 XML</a></li>
<li><a href="http://www.w3schools.com/xpath/xpath_syntax.asp">XPath Syntax</a></li>
<li><a href="http://www.evagoras.com/2011/02/10/improving-an-xml-feed-display-through-css-and-xslt/">Improving an XML feed through CSS and XSLT</a></li>
<li><a href="http://www.json.org/">JSON reference with links to parsers</a></li>
</ul>


<h2>Relational Databases, SQL, and NoSQL</h2>

<p>This shouldn&rsquo;t really come as a surprise to anyone. If you&rsquo;re going to analyse information, you&rsquo;re going to need to deal with databases. An analyst ought to be able to write <abbr title="Structured Query Language">SQL</abbr>, understand normalisation/ denormalisation, and possibly also have some knowledge of <abbr title="Object-Relational Mapping">ORM</abbr>. NoSQL databases are also useful when the data structure is better represented by document, graph or key:value structures (and where tables would be very sparse).</p>

<ul>
<li><a href="http://dev.mysql.com/">MySQL</a></li>
<li><a href="http://www.nparikh.org/unix/mysql.php">MySQL Cheat Sheet</a></li>
<li><a href="http://www.sqlite.org/">SQLite</a></li>
<li><a href="http://en.wikipedia.org/wiki/Database_normalization">database normalisation</a> and <a href="http://www.codinghorror.com/blog/2008/07/maybe-normalizing-isnt-normal.html">denormalisation</a></li>
<li><a href="http://nosql-database.org/">NoSQL Guide</a></li>
<li><a href="http://www.mongodb.org/">Mongodb</a> &ndash; document database</li>
<li><a href="http://www.neo4j.org/">Neo4j</a> &ndash; graph database</li>
<li><a href="http://couchdb.apache.org/">CouchDB</a> &ndash; json + map reduce + http</li>
</ul>


<h2>Statistical Library</h2>

<p>Unless you really want to code algorithms for every statistical process from scratch, you&rsquo;re going to want to adopt and familiarise yourself with a statistical library. I would strongly recommend <code>GNU-R</code> although other options include <code>Stata</code>, <code>SPSS</code>, and <code>Matlab</code> etc. Of course there is probably a library available for your prefered programming language. <code>scipy</code> and <code>numpy</code> are very popular choices for <code>python</code>.</p>

<ul>
<li><a href="http://www.r-project.org/">The R Project</a></li>
<li><a href="http://rseek.org">R Seek</a> &ndash; R-specific search engine</li>
<li><a href="http://stackoverflow.com/tags/r">R tag on Stackoverflow</a> (bonus: <a href="http://stats.stackexchange.com/">Cross Validated</a> &ndash; the stats-specific Stack Exchange Q&amp;A site)</li>
<li><a href="http://blog.revolutionanalytics.com/">Revolution Analytics Blog</a></li>
<li><a href="http://docs.ggplot2.org/current/">ggplot2</a></li>
<li><a href="gretl.sourceforge.net">gretl</a> &ndash; Gnu Regression, Econometrics and Time-series Library</li>
</ul>


<h2>Visualisation Framework</h2>

<p>Similarly you&rsquo;ll probably want to use some sort of framework for generating visualisations. I&rsquo;ve tried many and have found myself coming back to a few:</p>

<ul>
<li><a href="http://d3js.org">d3 data driven documents</a> &ndash; A JavaScript library for manipulating documents based on data using HTML, SVG and CSS</li>
<li><a href="http://docs.ggplot2.org/current/">ggplot2</a> &ndash; the tool that usually attracts people to R; excellent for quickly sketching out data graphics</li>
<li><a href="http://processing.org">processing</a> &ndash; Processing is an electronic sketchbook for developing ideas &ndash; in Java and now Javascript too.</li>
</ul>


<h2>Report Templating</h2>

<p>Usually you&rsquo;re going to want to place your analysis in context with some commentary. If you&rsquo;re going to be repeating a given report &ndash; either in part or in full &ndash; they you stand to benefit greatly from using some form of template. Again this will depend upon your prefered programming languages. My favoured tools are listed below.</p>

<ul>
<li><a href="http://www.stat.uni-muenchen.de/~leisch/Sweave/">Sweave</a> &ndash; R + Latex and <a href="http://rss.acs.unt.edu/Rdoc/library/odfWeave/html/odfWeave.html">Open Document Format</a>. I&rsquo;ve not tried it yet but <a href="http://yihui.name/knitr/">Knitr</a> looks like it could be a useful package).</li>
<li><a href="http://haml.info/">HAML &ndash; HTML abstraction markup language</a> &ndash; very clear markup with indentation (so you don&rsquo;t have to write out every tag twice), you&rsquo;re able to use ruby inline.</li>
<li><a href="http://sass-lang.com">Sass &ndash; Syntactically Awesome Stylesheets</a> &ndash; an extension of CSS3, adding nested rules, variables, mixins, selector inheritance, and more.</li>
<li><a href="http://garann.github.com/template-chooser/">Javascript Template Engine Chooser</a></li>
<li><a href="http://d3js.org">d3 data driven documents</a> &ndash; for programmatically creating html from data</li>
</ul>


<h2>Version Management</h2>

<p>Version management is vital for tracking progress, making experimental branches in your work, and coordinating across teams. The clear leader in this regard is <a href="http://git-scm.com">git</a> and the amazing project hosting environment <a href="https://github.com/">github</a>. I&rsquo;ve got in the habit of versioning everything &ndash; not just my code &ndash; but my home file configurations too.</p>

<h2>And the rest&hellip;</h2>

<p>This is just a quick list I wrote off the top of my head. What have I missed? What else do you use?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Modest Evaluation and the Burden of Proof]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/11/evaluation-and-the-burden-of-proof/"/>
    <updated>2012-12-11T16:04:00+01:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/11/evaluation-and-the-burden-of-proof</id>
    <content type="html"><![CDATA[<p>Impact evaluation studies are often called upon to put a price on something that is impossible to value. Evaluators ought to take this limitation seriously when designing their research methods. This post discusses the problem and suggests some responses.</p>

<!-- more -->


<p>The <strong>burden of proof</strong> is said to lie with the party making a claim. In order to convince someone of some truth, you are obliged to support your claim with evidence. The audience hearing your claim will make a judgement by scrutinising the evidence you present in support.</p>

<p>This obligation should not be treated lightly. In some cases, it isn&rsquo;t possible to marshall evidence to support our intuitions. Indeed we can&rsquo;t always know what we don&rsquo;t know and so we shouldn&rsquo;t act like we can. Recognition of this limitation is known as <strong>epistemological modesty</strong>. This perspective doesn&rsquo;t mean that we have to admit defeat. Instead it is basis for action. Action that takes place with an awareness of our necessarily limited understanding.</p>

<p>This approach has important lessons for evaluation. It suggests that we should avoid the temptation to reduce the nuances of impact evaluation to a single result (i.e. an absolute figure in pounds sterling for the overall impact or a summary cost-benefit ratio). Although it might seem appealing to be able to boast about a large absolute impact figure or to point to a high return on investment, the evidence required to support those definite figures may not be credible. We should not invite the audience to question and undermine the claims being made. Instead we ought to make modest claims that allow for the greatest degree of flexibility over the evidence. <strong>It&rsquo;s better to be confident in a modest conclusion than unconfident in a bold one</strong>. Our efforts should be focussed on validating our assumptions and understanding situations under which our conclusions might be undermined. Thus the burden of proof is lightened and the claims more credibile.</p>

<p>Our ability to do this will, of course, depend upon the circumstance but examples of how this might work in practice include:</p>

<ul>
<li>establishing the minimum level of evidence required to demonstrate that benefits are at least equal to costs (rather than straining evidence to calculate the maximum plausible ratio of benefits to costs),</li>
<li>comparing projects on the basis of the credibility of the evidence required to demonstrate that break-even position,</li>
<li>deriving relative rather than absolute measures (so that we be certain that the comparisons are fair and reasonable,)</li>
<li>using confidence-intervals rather than point-estimates to make inferences, and</li>
<li>using evaluation to guide research (to identify otherwise implicit assumptions that need to be validated).</li>
</ul>

]]></content>
  </entry>
  
</feed>
