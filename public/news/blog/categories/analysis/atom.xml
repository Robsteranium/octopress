<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: analysis | Infonomics Blog]]></title>
  <link href="http://infonomics.ltd.uk/news/blog/categories/analysis/atom.xml" rel="self"/>
  <link href="http://infonomics.ltd.uk/news/"/>
  <updated>2014-01-31T09:59:45+01:00</updated>
  <id>http://infonomics.ltd.uk/news/</id>
  <author>
    <name><![CDATA[Robin Gower]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sonification and Auditory Display Primer]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2014/01/19/sonification-and-auditory-display-primer/"/>
    <updated>2014-01-19T18:36:00+01:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2014/01/19/sonification-and-auditory-display-primer</id>
    <content type="html"><![CDATA[<p>An auditory display uses sound to convery information. Sonification is defined as a type auditory display that uses non-speech audio, rendering sound in response to data and interaction.</p>

<p>This post provides a brief introduction to sonfication based upon <a href="http://sonification.de/handbook/index.php/chapters/chapter2/">Chapter 2</a> of the <a href="http://sonification.de/handbook/">Sonification Handbook</a>.</p>

<div class="well">
  <p>A <a href="http://sonify.psych.gatech.edu/research/soundscapes/">soundscape mapping a stock market index to ecological sounds</a>. As the index rises, bird calls, crickets, frogs, and other forest sounds are added. As the index falls, rain and thunder are heard. The soundscape is designed to be monitoring peripherally and not to be intrusive.</p>
  <p><audio src="http://sonify.psych.gatech.edu/research/soundscapes/sounds/fullsample01.mp3" type="audio/mpeg" controls></audio></p>
</div>




<!--more-->


<h2>Why sonification is useful</h2>

<div class="well callout">
  <p><a href="http://silentlistening.wordpress.com/2008/05/09/dispersion-of-sound-waves-in-ice-sheets/">Andreas Bick's field recording of ice cracks</a> is already in the audible range. The recording exhibits the difference between the transmission of the echoes through ice and water.</p>
  <p><audio src="http://silentlistening.files.wordpress.com/2008/04/frost-pattern.mp3" controls></audio></p>
</div>


<ul>
<li>the human auditory system is good at recognising temporal changes or patterns</li>
<li>the operator may not always be able to see a visual display

<ul>
<li>the content does not require constant observation such as warning alarms</li>
<li>an existing visual system may be overloaded</li>
<li>the perceiver may be visually impaired</li>
</ul>
</li>
<li>the data is verbal-categorical, has a high number of dimensions, or requires rapid detection</li>
<li>because the rise of mobile devices mean smaller screens and less room for visual displays</li>
</ul>


<h2>Use cases</h2>

<div class="well callout">
  <p>An auditory menu designed for use by drivers. You can hear the user scrolling through an address book. The spindex cues give an overview of alphabetical position, with slower navigation triggering spearcons for each contact.</p>
  <p><audio src="http://sonification.de/handbook/media/chapter2/SHB-S2.2.mp3" type="audio/mpeg" controls></audio></p>
</div>


<ul>
<li>alerts and notifications (indicating an occurance)</li>
<li>alarms and warnings (indicating an adverse occurance, perhaps requiring an urgent response)</li>
<li>status and progress indicators</li>
<li>data exploration and auditory graphs</li>
<li>art, entertainment, sport and leisure</li>
</ul>


<h2>Types of Sonification</h2>

<h3>Interaction</h3>

<div class="well callout">
<p>
<img src="http://infonomics.ltd.uk/news/images/post_images/tectonics.png" alt="Tectonics thumbnail" />
Florian Dombois' audification of <a href="http://www.auditory-seismology.org/version2004/tectonics.html">Plate Tectonics</a> demonstrates the difference between the plop of the parting Atlantic ocean plates and the crack of the plates drifting against each other).
</p>
</div>


<p>At one end of the scale, there are non-interactive sonifications that once triggered play in their entirety as a concert or a tour around the information. This has parallels with the direction instruction method of teaching whereby an existing conclusion or viewpoint is demonstrated.</p>

<p>At the other end of the scale, there are user-initiated sonifications that require the user to engage in a conversation. This has parallels with the enquiry-based learning method which begins with questions, problems, or scenarios allowing knowledge to be discovered through exploration.</p>

<p>Somewhere in between lies the facility for manipulation of the sonification at basic level &ndash; controlling the speed, pausing, fast-forwarding, and/ or rewinding.</p>

<h3>Methods</h3>

<ul>
<li><em>Audification</em> transforms periodic or other data that has a waveform structure into the audible range.</li>
<li><em>Parameter Mapping Sonification</em> extends this to other data forms, mapping a data dimension to an accoustic dimension</li>
<li><em>Model-based Sonfication</em> emerges from the interaction of a user with an instrument so that the data structure is understood through the sonic responses of a virtual object.</li>
</ul>


<p>An <em>auditory icon</em> represents something and bears an analogous resemblence so that it should be understood without explanation. An <em>earcon</em> is more symbolic, and has and arbitrary mapping of sound to meaning. Although Earcons are more flexible, they may have to be learned. The <em>spearcon</em>, a speech earcon, has the potential to offer the best of both worlds &ndash; flexibility and understandability. A <em>spindex</em> is a set of brief speech sounds that are used to navigate a long menu.</p>

<h3>Design Considerations</h3>

<p>Qualitative data may be better represented by dimensions of sound that are perceived as categorical, such as timbre. Whereas pitch or loudness, which are more continuous, may be better for ratio or interval data.</p>

<p>The polarity of the mapping matters. In one study, listeners agreed that pitch should increase with increasing temperature but that it should decrease with increasing size.</p>

<p>While the human hearing range stretches from about 20 Hz to 20,000 Hz, it may be more successful to scale data to the range where hearing is most sensitive, for example between 1000-5000 Hz.</p>

<p>A musical model, for example the notes on a piano, can provide a scale with perceptually equal steps. This convenience does come at the cost of resolution. A MIDI display using only notes 35-100 provides 65 points whereas a microtonal scale would be comparatively infinite.</p>

<p>Monitoring tasks require that the listener has a priori knowledge of a particular template so that they may recognise a sound and it&rsquo;s meaning.</p>

<p>Concurrent presentation of multiple data streams requires that the user be able to segregate the streams. Differences in timbre (musical instrument) or spatial separation (stereo panning) have been used for this purpose.</p>

<p>Context cues can aid perception. Like tick marks on the axis of a visual graph, a series of clicks can help the user keep track of time and a repeating reference tone can help with point estimation.</p>

<div class="well">
<p>The <a href="http://www.sonifyer.org/sound/erdbewegung/progress/?id=38">Sonification of Tohoku Earthquake</a> accelerates seismic activiy by a factor of 1440 to bring signal into the audible range.</a></p>
<div id="ytplayer"></div>
<script>
  // Load the IFrame Player API code asynchronously.
  var tag = document.createElement('script');
  tag.src = "https://www.youtube.com/player_api";
  var firstScriptTag = document.getElementsByTagName('script')[0];
  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

  // Replace the 'ytplayer' element with an <iframe> and
  // YouTube player after the API code downloads.
  var player;
  function onYouTubePlayerAPIReady() {
    player = new YT.Player('ytplayer', {
      height: '390',
      width: '640',
      videoId: '3PJxUPvz9Oo'
    });
  }
</script>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LEP Economic Profiles - How do local assets affect productivity]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2014/01/17/lep-economic-profiles-how-do-local-assets-affect-productivity/"/>
    <updated>2014-01-17T08:31:00+01:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2014/01/17/lep-economic-profiles-how-do-local-assets-affect-productivity</id>
    <content type="html"><![CDATA[<p>Jim Twomey of <a href="http://www.pion-economics.co.uk/">Pion Economics</a> put together a fascinating statistical analysis of Local Economic Partnership areas. I offered to visualise the findings, and here are the results (click through for the interactive visualisation):</p>

<p><a href="http://infonomics.ltd.uk/lep-profiles/index.html"><img src="/images/post_images/lep_analysis_screenshot.png" title="LEP Analysis Screenshot" ></a></p>

<!-- more -->


<p>The statistical techqniue is quite advanced, and I&rsquo;m still searching for a simple way to explain it. The objective is to describe differences in productivity in terms of the attributes of each place so that performance may be understood, and insights found into what a place might need to improve. First the attributes are gathered together into asset groups that correlate closely with one another (factor analysis). Then the level of these 8 groups is compared against local economic productivity to estimate the contribution of those assets to performance. More information on the technique may be found in the <a href="http://pion-economics.co.uk/cms/resources/uploads/File/LEP_document.pdf">Developing LEP Economies Report</a>.</p>

<p>The interactive visualisation is designed to provide both an overview of the patterns across LEP areas and a means of finding the results that apply to each LEP individually. The analysis presents three panels. The map on the left is for navigation. By hovering over a given place, the relevant LEP profile will be selected. On the right, the top panel presents all LEPs ordered by productivity level and the bottom panel the contribution of assets. Each chart shows the overall range of values for context, with the selected LEP area highlighted in bold. You may also navigate using the top right panel.</p>

<p>The interactive element allows you to explore patterns and relationships. Try, for example, drawing your cursor from the periphery of England in towards London. Note how the level of productivity and the contribution of the Access asset group changes. Try comparing rural and urban areas.</p>

<p>We found that this visualisation technique makes it far more efficient to present research findings (displaying 39 profiles in one graphic) and allows additional insights to be discovered through interactive exploration.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to choose a Sample Size]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/10/18/how-to-choose-a-sample-size/"/>
    <updated>2013-10-18T09:26:00+02:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/10/18/how-to-choose-a-sample-size</id>
    <content type="html"><![CDATA[<p>I&rsquo;m often asked to provide advice on this question. The short answer is usually &ldquo;it depends&rdquo;. This is the longer answer that explains what it depends on!</p>

<p>The size of a sample doesn&rsquo;t need to be as large people usually think. The sample size doesn&rsquo;t actually depend upon the population size (i.e. the things &ndash; individuals, organisations, or projects &ndash; that the sample is supposed to represent). Indeed National opinion polls usually include no more than 1,000 responses (although they do require an <a href="http://en.wikipedia.org/wiki/Sampling_bias#Historical_examples">unbiased sample</a>).</p>

<p>So what does sample size depend upon..?</p>

<!-- more -->


<h2>Why choosing a sample size is important</h2>

<p>A sample is a subset of a population that you wish to study. A sample is typically chosen for research because it is too difficult or expensive to talk to the whole population. Fortunately statistical theory demonstrates that it is possible for responses taken from a sample to represent a wider population.</p>

<p>This post explains some of things to consider when choosing a representative sample.</p>

<h2>Total sample size</h2>

<p>The choice of overall sample size is driven by three statistical considerations</p>

<ul>
<li>what effect size you want to capture (i.e. the expected difference between the things you&rsquo;re comparing),</li>
<li>how willing you are to miss effects that are present, and</li>
<li>how willing you are to mistakenly identify effects that aren&rsquo;t present.</li>
</ul>


<p>We can set the latter two according to statistical convention (significance level of 95%, and a statistical power of 80%). Effect size really depends upon the context.</p>

<p>The main unknown here will be the standard deviation &ndash; that is to say the extent to which the things you&rsquo;re measuring vary from case-to-case. This, of course, determines the degree to which we can draw conclusions about the whole population by looking at a sample.</p>

<p>We define the effect size relative to the standard deviation. The larger the effect size, the smaller the sample needed to detect it. Drawing on social research, Cohen suggests that 20%, 50%, and 80% (of the standard deviation) represent small, medium, and large effect sizes respectively.</p>

<h2>Representativeness of sub-groups</h2>

<p>There are a couple of issues here. First, the overall results should be representative (you don&rsquo;t want to miss-out certain groups) and unbiased (if the sample structure doesn&rsquo;t match the proportion of the overall population the results will need to be weighted). Second, you might want to compare the results of the sub-groups (e.g. treatment vs control groups, cohorts by gender/ age/ ethncitiy, or to see if results change over time/ by location).</p>

<p>Bear in mind the above discussion on sample sizes. The rule of thumb value for sub-samples is to have 30 responses &ndash; is consistent with an effect size of 73%. This would, of course, be exhaustive for some of the smaller sub-groups. You may need to combine sub-groups or deal with this variability in the analysis rather than in the sample design &ndash; e.g. removing outliers after the responses have been gathered). You might aim for 30 responses among the larger groups but relax this for smaller ones.</p>

<h2>Response rates</h2>

<p>This depends on how and who contacts the projects. Fortunately, as a funder you&rsquo;re likely to have their attention! In my experience of contacting beneficiaries on behalf of public/ social projects, a 30% response rate is good going. Don&rsquo;t be surprised if you receive a response rate lower than 1% for an email survey (although the ease of contacting a wide range should mean this is still an effective method). I&rsquo;d aim to contact about 5 times as many people as you need responses.</p>

<p>The overall decision on sample size ought to take into account the cost and difficulty of acquiring responses and the observed variability of responses. You might want to bootstrap the process by running an initial small sample that would serve to identify whether further booster samples are required (e.g. &ldquo;the overall result is satisfactory but we&rsquo;re concerned about the apparent discrepancy among very large projects so they&rsquo;ll be the subject of further research&rdquo;). Bear in mind that this is by no means a precise science and this guidance should be considered in the context of practical concerns. Perhaps the most useful contribution of statistical advice &ndash; rather than identifying a magic number &ndash; is to help you to understand the overall patterns (variability matters, diminishing returns apply, very large/ small projects might need to be considered separately).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[9 Tools Every Information Analyst Should Have]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/01/30/9-tools-every-information-analyst-should-have/"/>
    <updated>2013-01-30T12:15:00+01:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/01/30/9-tools-every-information-analyst-should-have</id>
    <content type="html"><![CDATA[<p>I was interested to read Rebecca Murphey&rsquo;s <a href="http://rmurphey.com/blog/2012/04/12/a-baseline-for-front-end-developers/">Baseline for Front End Developers</a>  in which the author realizes that she has begun to take for granted a basic set of tools of the trade. She presents &ldquo;a few things that [she] wants to start expecting people to be familiar with, along with some resources you can use if you feel like you need to get up to speed&rdquo;. I&rsquo;d like to pay homage to that article and revise it in light of <em>the tools I believe are essential for an Information Analyst</em>. There is considerable overlap in the tools suggested although the application will differ.</p>

<!--more -->


<p><img class="<a" src="href="http://imgs.xkcd.com/comics/regular_expressions.png">http://imgs.xkcd.com/comics/regular_expressions.png</a>" title="&lsquo;Wait, forgot to escape a space.  Wheeeeee[taptaptap]eeeeee.&rsquo; &lsquo;XKCD Regular Expressions Comic&rsquo;" ></p>

<h2>GNU/ UNIX Command Line Tools</h2>

<p>These tools are especially powerful because they don&rsquo;t require you to load the entire file into memory. This makes it possible to work with very large files efficiently.</p>

<ul>
<li><code>head</code>, <code>tail</code>, <code>less</code>, <code>cat</code> etc for looking at files</li>
<li><code>awk</code>, <code>sed</code>, and <code>tr</code> for processing and altering files</li>
<li><code>cut</code>, <code>paste</code>, and <code>join</code> again for editing and combining text files</li>
<li><code>sort</code> and <code>uniq</code></li>
<li><code>wc</code> for counting lines/ words/ characters</li>
<li><code>iconv</code> for converting between different encodings</li>
</ul>


<p>Some useful resources include:</p>

<ul>
<li><a href="http://blog.sanctum.geek.nz/series/unix-as-ide/">Unix as an IDE</a></li>
<li><a href="http://www.ibm.com/developerworks/linux/library/l-textutils/index.html">Simplify data extraction using Linux text utilities</a></li>
<li><a href="http://tldp.org/LDP/abs/html/textproc.html">Text Processing Commands in the Advanced Bash-Scripting Guide</a></li>
<li><a href="http://funarg.nfshost.com/r2/notes/sed-return-comma.html">Replacing returns with commas in Sed</a></li>
<li><a href="https://github.com/robbyrussell/oh-my-zsh">Oh my zsh</a> &ndash; zshell a replacement for bash</li>
</ul>


<h2>Regular Expressions</h2>

<p>Regexs are extremely powerful, if sometimes counter-intuitive. They are vital for defining patterns and replacements in text processing. Although some <a href="http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454">debate</a> exists a to whether they should be used to parse structured formats like XHTML, they are certainly a vital tool.</p>

<p>Having said which, I find this quote pretty amusing!
<blockquote><p>Some people, when confronted with a problem, think “I know, I&rsquo;ll use regular expressions.” Now they have two problems.</p><footer><strong>Jamie Zawinski <a href="http://regex.info/blog/2006-09-15/247">http://regex.info/blog/2006-09-15/247</a> Jeffrey Friedl&rsquo;s Blog</strong></footer></blockquote></p>

<ul>
<li><a href="http://gskinner.com/RegExr/">RegExr</a> &ndash; Regex building and testing tool</li>
<li><a href="http://www.regexper.com/">Regexper</a> &ndash; visualisation tool</li>
<li><a href="http://www.regular-expressions.info/">Reference Docs</a></li>
<li>An example for <a href="http://www.softwareprojects.com/resources/programming/t-validate-a-credit-card-1682.html">validating credit card numbers</a> or <a href="http://stackoverflow.com/a/164994/187009">validating UK Postcodes</a></li>
</ul>


<h2>A Programming/ Scripting Language</h2>

<p>If nothing else, these provide the glue to connect different sections of your data pipeline. The incredible range of libraries avavailable to most scripting languages will make quick work of many common information processing tasks: parsing, spidering, <abbr title="Extraction, Translation, and Loading">ETL</abbr> etc. The choice of language is down to personal preference or compatibility with the team.
It should suffice to say that knowledge of a scripting language is necessary and I don&rsquo;t feel the need to recommend a particular language. Having said which, I can&rsquo;t resist saying that I prefer <code>ruby</code> but have dabbled in <code>perl</code> and <code>python</code>. I find the following ruby gems indispensible: <code>mechanize</code>, <code>nokogiri</code>, <code>fastercsv</code> (now <code>CSV</code> in the 1.9 standard library), and <code>json</code>. I&rsquo;ve also been using <code>javascript</code> and node for scripting outside of the browser as asynchronous processing has proved to be very quick (if a little peculiar to code at first).</p>

<ul>
<li><a href="http://mislav.uniqpath.com/poignant-guide/book/chapter-1.html">Why The Lucky Stiff&rsquo;s (poignant) Guide to Ruby</a></li>
<li><a href="http://www.rubyist.net/~slagell/ruby/">A guide written by Matz (the original author of Ruby)</a></li>
<li><a href="https://github.com/styleguide/ruby">Github&rsquo;s Ruby Style Guide</a></li>
<li><a href="https://www.ruby-toolbox.com/">Ruby Toolbox</a> &ndash; for finding and selecting gems</li>
<li>John Ressig&rsquo;s <a href="http://ejohn.org/apps/learn/">Learning Advanced Javascript</a></li>
</ul>


<h2>Text data files</h2>

<p>Probably the most basic technology used in information analysis, the flat file is the lowest common denominator. You can almost guarantee that everyone will be able to cope with <code>csv</code> files (even if they need to be translated before loading into another tool). The expressiveness and rigour of <code>XML</code> makes it another vital tool although it&rsquo;s rapidly being superceded in bandwidth-conscious realm of http by the less verbose <code>json</code>.
Information analysts ought to be able to handle (read, understand, parse and translate) files in these all of formats even if they immediately convert them to a prefered type.</p>

<ul>
<li><a href="http://www.delicious.com/redirect?url=http%3A//andrewjwelch.com/code/xslt/csv/csv-to-xml_v2.html">XSLT for CSV 2 XML</a></li>
<li><a href="http://www.w3schools.com/xpath/xpath_syntax.asp">XPath Syntax</a></li>
<li><a href="http://www.evagoras.com/2011/02/10/improving-an-xml-feed-display-through-css-and-xslt/">Improving an XML feed through CSS and XSLT</a></li>
<li><a href="http://www.json.org/">JSON reference with links to parsers</a></li>
</ul>


<h2>Relational Databases, SQL, and NoSQL</h2>

<p>This shouldn&rsquo;t really come as a surprise to anyone. If you&rsquo;re going to analyse information, you&rsquo;re going to need to deal with databases. An analyst ought to be able to write <abbr title="Structured Query Language">SQL</abbr>, understand normalisation/ denormalisation, and possibly also have some knowledge of <abbr title="Object-Relational Mapping">ORM</abbr>. NoSQL databases are also useful when the data structure is better represented by document, graph or key:value structures (and where tables would be very sparse).</p>

<ul>
<li><a href="http://dev.mysql.com/">MySQL</a></li>
<li><a href="http://www.nparikh.org/unix/mysql.php">MySQL Cheat Sheet</a></li>
<li><a href="http://www.sqlite.org/">SQLite</a></li>
<li><a href="http://en.wikipedia.org/wiki/Database_normalization">database normalisation</a> and <a href="http://www.codinghorror.com/blog/2008/07/maybe-normalizing-isnt-normal.html">denormalisation</a></li>
<li><a href="http://nosql-database.org/">NoSQL Guide</a></li>
<li><a href="http://www.mongodb.org/">Mongodb</a> &ndash; document database</li>
<li><a href="http://www.neo4j.org/">Neo4j</a> &ndash; graph database</li>
<li><a href="http://couchdb.apache.org/">CouchDB</a> &ndash; json + map reduce + http</li>
</ul>


<h2>Statistical Library</h2>

<p>Unless you really want to code algorithms for every statistical process from scratch, you&rsquo;re going to want to adopt and familiarise yourself with a statistical library. I would strongly recommend <code>GNU-R</code> although other options include <code>Stata</code>, <code>SPSS</code>, and <code>Matlab</code> etc. Of course there is probably a library available for your prefered programming language. <code>scipy</code> and <code>numpy</code> are very popular choices for <code>python</code>.</p>

<ul>
<li><a href="http://www.r-project.org/">The R Project</a></li>
<li><a href="http://rseek.org">R Seek</a> &ndash; R-specific search engine</li>
<li><a href="http://stackoverflow.com/tags/r">R tag on Stackoverflow</a> (bonus: <a href="http://stats.stackexchange.com/">Cross Validated</a> &ndash; the stats-specific Stack Exchange Q&amp;A site)</li>
<li><a href="http://blog.revolutionanalytics.com/">Revolution Analytics Blog</a></li>
<li><a href="http://docs.ggplot2.org/current/">ggplot2</a></li>
<li><a href="gretl.sourceforge.net">gretl</a> &ndash; Gnu Regression, Econometrics and Time-series Library</li>
</ul>


<h2>Visualisation Framework</h2>

<p>Similarly you&rsquo;ll probably want to use some sort of framework for generating visualisations. I&rsquo;ve tried many and have found myself coming back to a few:</p>

<ul>
<li><a href="http://d3js.org">d3 data driven documents</a> &ndash; A JavaScript library for manipulating documents based on data using HTML, SVG and CSS</li>
<li><a href="http://docs.ggplot2.org/current/">ggplot2</a> &ndash; the tool that usually attracts people to R; excellent for quickly sketching out data graphics</li>
<li><a href="http://processing.org">processing</a> &ndash; Processing is an electronic sketchbook for developing ideas &ndash; in Java and now Javascript too.</li>
</ul>


<h2>Report Templating</h2>

<p>Usually you&rsquo;re going to want to place your analysis in context with some commentary. If you&rsquo;re going to be repeating a given report &ndash; either in part or in full &ndash; they you stand to benefit greatly from using some form of template. Again this will depend upon your prefered programming languages. My favoured tools are listed below.</p>

<ul>
<li><a href="http://www.stat.uni-muenchen.de/~leisch/Sweave/">Sweave</a> &ndash; R + Latex and <a href="http://rss.acs.unt.edu/Rdoc/library/odfWeave/html/odfWeave.html">Open Document Format</a>. I&rsquo;ve not tried it yet but <a href="http://yihui.name/knitr/">Knitr</a> looks like it could be a useful package).</li>
<li><a href="http://haml.info/">HAML &ndash; HTML abstraction markup language</a> &ndash; very clear markup with indentation (so you don&rsquo;t have to write out every tag twice), you&rsquo;re able to use ruby inline.</li>
<li><a href="http://sass-lang.com">Sass &ndash; Syntactically Awesome Stylesheets</a> &ndash; an extension of CSS3, adding nested rules, variables, mixins, selector inheritance, and more.</li>
<li><a href="http://garann.github.com/template-chooser/">Javascript Template Engine Chooser</a></li>
<li><a href="http://d3js.org">d3 data driven documents</a> &ndash; for programmatically creating html from data</li>
</ul>


<h2>Version Management</h2>

<p>Version management is vital for tracking progress, making experimental branches in your work, and coordinating across teams. The clear leader in this regard is <a href="http://git-scm.com">git</a> and the amazing project hosting environment <a href="https://github.com/">github</a>. I&rsquo;ve got in the habit of versioning everything &ndash; not just my code &ndash; but my home file configurations too.</p>

<h2>And the rest&hellip;</h2>

<p>This is just a quick list I wrote off the top of my head. What have I missed? What else do you use?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[3 Metrics for Heath Economics Analysis]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/19/3-metrics-for-heath-economics-analysis/"/>
    <updated>2012-12-19T16:43:00+01:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/19/3-metrics-for-heath-economics-analysis</id>
    <content type="html"><![CDATA[<p>I&rsquo;m often asked to suggest measures for assessing health interventions from an economic perspective. Here are explanations of four commonly used analytical methods:</p>

<ul>
<li>Reduction in Costs of Treatment</li>
<li>Cost per Quality Adjusted Life Years (QALY)</li>
<li>Incremental Cost Effectiveness Ratio (ICER)</li>
</ul>


<!--more-->


<p>As should be obvious from the titles, cost plays a significance role in health economics analysis. The first measure considers cost directly. The second measures also incorporate utility or benefit. The final measure should be used for interpretation.</p>

<p>In practice, it&rsquo;s advisable to consider using more than one method to account for the limitations of individual measures. It&rsquo;s also important to control for demographic and socio-economic characteristics such as age, gender, ethnicity, employment, and health condition. It will also be insightful to consider location as a proxy indicator for a range of (highly-correlated) socio-economic characteristics.</p>

<h2>Reduction in costs of treatment</h2>

<p>This is the most easily understandable economic impact of health interventions. If preventive steps are taken, fewer people contract disease, and less will need to be spent on treatment. The main focus of research under this measure is in <strong>quantifying the intervention&rsquo;s impact on health and the cost of treatment that is obviated</strong>. Weight loss, for example, will reduce the relative risk of diabetes and cardio-vascual disease. The relationship between intervention and disease risk might be established by reference to physiological measures (such as Body Mass Index). Cost of treatment might include the cost of prescription medicine, GP consultation, surgery and palliative care.</p>

<p>The difficulty with this sort of measure is that the impact is stated in terms of money that would otherwise have had to be spent and, as such, is quite intangigble.</p>

<h2>Cost per QALY</h2>

<p>A quality adjusted-life year, or QALY, represents <strong>the number of years of life that would be added by an intervention, taking into account the quality of those years</strong>. If the extra years would not be lived in full health they are given a value less than 1. Clearly the method of adjustment for quality is critical to this metric.
A commonly adopted measure is the <a href="http://www.euroqol.org/">EQ-5D</a> scale which seeks to categorise health states according to 5 dimensions: mobility, self-care, usual activities, pain/ discomfort and anxiety/ depression. This is combined with a quantiative measure of the patients self-assessment of their health on a <em>vertical analogue scale</em>.
Other approaches include the <em>time trade-off</em>, where respondents are asked to choose between remaining in an ill state or being restored to perfect health with a shorter life expectancy, and <em>standard gamble</em> where the alternative could restore them to perfect health or kill them.
This measure is criticised due to the difficulty of establishing a meaningful, objective, and comparable definition of &ldquo;perfect health&rdquo; and &ldquo;disease burden&rdquo;.</p>

<h2>Incremental Cost-Effectiveness Ratio</h2>

<p>This ratio is used for comparing intervention options on a like for like basis. The Incremental Cost Effectiveness Ratio (ICER) is a <strong>comparison of the relative cost per QALY of the intervention and a reference case</strong> as follows (where QALY refers to a Quality Adjusted Life Year):
$$
ICER = \frac{ \text{£ Cost of intervention} &ndash; \text{£ Cost of reference case} } { \text{QALY Effect of intervention} –  \text{QALY Effect of reference case} }
$$
The ICER assumes a multiplicative model for QALYs (i.e. a change in scale will change the ICER ratio even though original figures remain the same).
Benchmarking against other interventions provides valuable contextual information. The <a href="https://research.tufts-nemc.org/cear4/default.aspx">Tufts Cost-Effectiveness Analysis Registry</a> is a valuable resource in this regard.</p>

<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

]]></content>
  </entry>
  
</feed>
