<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: information | Infonomics Blog]]></title>
  <link href="http://infonomics.ltd.uk/news/blog/categories/information/atom.xml" rel="self"/>
  <link href="http://infonomics.ltd.uk/news/"/>
  <updated>2013-03-20T18:23:12+00:00</updated>
  <id>http://infonomics.ltd.uk/news/</id>
  <author>
    <name><![CDATA[Robin Gower]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[9 Tools Every Information Analyst Should Have]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/01/30/9-tools-every-information-analyst-should-have/"/>
    <updated>2013-01-30T12:15:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/01/30/9-tools-every-information-analyst-should-have</id>
    <content type="html"><![CDATA[<p>I was interested to read Rebecca Murphey's <a href="http://rmurphey.com/blog/2012/04/12/a-baseline-for-front-end-developers/">Baseline for Front End Developers</a>  in which the author realizes that she has begun to take for granted a basic set of tools of the trade. She presents "a few things that [she] wants to start expecting people to be familiar with, along with some resources you can use if you feel like you need to get up to speed". I'd like to pay homage to that article and revise it in light of <em>the tools I believe are essential for an Information Analyst</em>. There is considerable overlap in the tools suggested although the application will differ.</p>

<!--more -->


<p><img src="http://imgs.xkcd.com/comics/regular_expressions.png" title="Wait, forgot to escape a space.  Wheeeeee[taptaptap]eeeeee." alt="XKCD Regular Expressions Comic"></p>

<h2>GNU/ UNIX Command Line Tools</h2>

<p>These tools are especially powerful because they don't require you to load the entire file into memory. This makes it possible to work with very large files efficiently.</p>

<ul>
<li><code>head</code>, <code>tail</code>, <code>less</code>, <code>cat</code> etc for looking at files</li>
<li><code>awk</code>, <code>sed</code>, and <code>tr</code> for processing and altering files</li>
<li><code>cut</code>, <code>paste</code>, and <code>join</code> again for editing and combining text files</li>
<li><code>sort</code> and <code>uniq</code></li>
<li><code>wc</code> for counting lines/ words/ characters</li>
<li><code>iconv</code> for converting between different encodings</li>
</ul>


<p>Some useful resources include:</p>

<ul>
<li><a href="http://blog.sanctum.geek.nz/series/unix-as-ide/">Unix as an IDE</a></li>
<li><a href="http://www.ibm.com/developerworks/linux/library/l-textutils/index.html">Simplify data extraction using Linux text utilities</a></li>
<li><a href="http://tldp.org/LDP/abs/html/textproc.html">Text Processing Commands in the Advanced Bash-Scripting Guide</a></li>
<li><a href="http://funarg.nfshost.com/r2/notes/sed-return-comma.html">Replacing returns with commas in Sed</a></li>
<li><a href="https://github.com/robbyrussell/oh-my-zsh">Oh my zsh</a> - zshell a replacement for bash</li>
</ul>


<h2>Regular Expressions</h2>

<p>Regexs are extremely powerful, if sometimes counter-intuitive. They are vital for defining patterns and replacements in text processing. Although some <a href="http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454">debate</a> exists a to whether they should be used to parse structured formats like XHTML, they are certainly a vital tool.</p>

<p>Having said which, I find this quote pretty amusing!
<blockquote><p>Some people, when confronted with a problem, think “I know, I'll use regular expressions.” Now they have two problems.</p><footer><strong>Jamie Zawinski</strong> <cite><a href='http://regex.info/blog/2006-09-15/247'>Jeffrey Friedl's Blog</a></cite></footer></blockquote></p>

<ul>
<li><a href="http://gskinner.com/RegExr/">RegExr</a> - Regex building and testing tool</li>
<li><a href="http://www.regexper.com/">Regexper</a> - visualisation tool</li>
<li><a href="http://www.regular-expressions.info/">Reference Docs</a></li>
<li>An example for <a href="http://www.softwareprojects.com/resources/programming/t-validate-a-credit-card-1682.html">validating credit card numbers</a> or <a href="http://stackoverflow.com/a/164994/187009">validating UK Postcodes</a></li>
</ul>


<h2>A Programming/ Scripting Language</h2>

<p>If nothing else, these provide the glue to connect different sections of your data pipeline. The incredible range of libraries avavailable to most scripting languages will make quick work of many common information processing tasks: parsing, spidering, <abbr title="Extraction, Translation, and Loading">ETL</abbr> etc. The choice of language is down to personal preference or compatibility with the team.
It should suffice to say that knowledge of a scripting language is necessary and I don't feel the need to recommend a particular language. Having said which, I can't resist saying that I prefer <code>ruby</code> but have dabbled in <code>perl</code> and <code>python</code>. I find the following ruby gems indispensible: <code>mechanize</code>, <code>nokogiri</code>, <code>fastercsv</code> (now <code>CSV</code> in the 1.9 standard library), and <code>json</code>. I've also been using <code>javascript</code> and node for scripting outside of the browser as asynchronous processing has proved to be very quick (if a little peculiar to code at first).</p>

<ul>
<li><a href="http://mislav.uniqpath.com/poignant-guide/book/chapter-1.html">Why The Lucky Stiff's (poignant) Guide to Ruby</a></li>
<li><a href="http://www.rubyist.net/~slagell/ruby/">A guide written by Matz (the original author of Ruby)</a></li>
<li><a href="https://github.com/styleguide/ruby">Github's Ruby Style Guide</a></li>
<li><a href="https://www.ruby-toolbox.com/">Ruby Toolbox</a> - for finding and selecting gems</li>
<li>John Ressig's <a href="http://ejohn.org/apps/learn/">Learning Advanced Javascript</a></li>
</ul>


<h2>Text data files</h2>

<p>Probably the most basic technology used in information analysis, the flat file is the lowest common denominator. You can almost guarantee that everyone will be able to cope with <code>csv</code> files (even if they need to be translated before loading into another tool). The expressiveness and rigour of <code>XML</code> makes it another vital tool although it's rapidly being superceded in bandwidth-conscious realm of http by the less verbose <code>json</code>.
Information analysts ought to be able to handle (read, understand, parse and translate) files in these all of formats even if they immediately convert them to a prefered type.</p>

<ul>
<li><a href="http://www.delicious.com/redirect?url=http%3A//andrewjwelch.com/code/xslt/csv/csv-to-xml_v2.html">XSLT for CSV 2 XML</a></li>
<li><a href="http://www.w3schools.com/xpath/xpath_syntax.asp">XPath Syntax</a></li>
<li><a href="http://www.evagoras.com/2011/02/10/improving-an-xml-feed-display-through-css-and-xslt/">Improving an XML feed through CSS and XSLT</a></li>
<li><a href="http://www.json.org/">JSON reference with links to parsers</a></li>
</ul>


<h2>Relational Databases, SQL, and NoSQL</h2>

<p>This shouldn't really come as a surprise to anyone. If you're going to analyse information, you're going to need to deal with databases. An analyst ought to be able to write <abbr title="Structured Query Language">SQL</abbr>, understand normalisation/ denormalisation, and possibly also have some knowledge of <abbr title="Object-Relational Mapping">ORM</abbr>. NoSQL databases are also useful when the data structure is better represented by document, graph or key:value structures (and where tables would be very sparse).</p>

<ul>
<li><a href="http://dev.mysql.com/">MySQL</a></li>
<li><a href="http://www.nparikh.org/unix/mysql.php">MySQL Cheat Sheet</a></li>
<li><a href="http://www.sqlite.org/">SQLite</a></li>
<li><a href="http://en.wikipedia.org/wiki/Database_normalization">database normalisation</a> and <a href="http://www.codinghorror.com/blog/2008/07/maybe-normalizing-isnt-normal.html">denormalisation</a></li>
<li><a href="http://nosql-database.org/">NoSQL Guide</a></li>
<li><a href="http://www.mongodb.org/">Mongodb</a> - document database</li>
<li><a href="http://www.neo4j.org/">Neo4j</a> - graph database</li>
<li><a href="http://couchdb.apache.org/">CouchDB</a> - json + map reduce + http</li>
</ul>


<h2>Statistical Library</h2>

<p>Unless you really want to code algorithms for every statistical process from scratch, you're going to want to adopt and familiarise yourself with a statistical library. I would strongly recommend <code>GNU-R</code> although other options include <code>Stata</code>, <code>SPSS</code>, and <code>Matlab</code> etc. Of course there is probably a library available for your prefered programming language. <code>scipy</code> and <code>numpy</code> are very popular choices for <code>python</code>.</p>

<ul>
<li><a href="http://www.r-project.org/">The R Project</a></li>
<li><a href="http://rseek.org">R Seek</a> - R-specific search engine</li>
<li><a href="http://stackoverflow.com/tags/r">R tag on Stackoverflow</a> (bonus: <a href="http://stats.stackexchange.com/">Cross Validated</a> - the stats-specific Stack Exchange Q&amp;A site)</li>
<li><a href="http://blog.revolutionanalytics.com/">Revolution Analytics Blog</a></li>
<li><a href="http://docs.ggplot2.org/current/">ggplot2</a></li>
<li><a href="gretl.sourceforge.net">gretl</a> - Gnu Regression, Econometrics and Time-series Library</li>
</ul>


<h2>Visualisation Framework</h2>

<p>Similarly you'll probably want to use some sort of framework for generating visualisations. I've tried many and have found myself coming back to a few:</p>

<ul>
<li><a href="http://d3js.org">d3 data driven documents</a> - A JavaScript library for manipulating documents based on data using HTML, SVG and CSS</li>
<li><a href="http://docs.ggplot2.org/current/">ggplot2</a> - the tool that usually attracts people to R; excellent for quickly sketching out data graphics</li>
<li><a href="http://processing.org">processing</a> - Processing is an electronic sketchbook for developing ideas - in Java and now Javascript too.</li>
</ul>


<h2>Report Templating</h2>

<p>Usually you're going to want to place your analysis in context with some commentary. If you're going to be repeating a given report - either in part or in full - they you stand to benefit greatly from using some form of template. Again this will depend upon your prefered programming languages. My favoured tools are listed below.</p>

<ul>
<li><a href="http://www.stat.uni-muenchen.de/~leisch/Sweave/">Sweave</a> - R + Latex and <a href="http://rss.acs.unt.edu/Rdoc/library/odfWeave/html/odfWeave.html">Open Document Format</a>. I've not tried it yet but <a href="http://yihui.name/knitr/">Knitr</a> looks like it could be a useful package).</li>
<li><a href="http://haml.info/">HAML - HTML abstraction markup language</a> - very clear markup with indentation (so you don't have to write out every tag twice), you're able to use ruby inline.</li>
<li><a href="http://sass-lang.com">Sass - Syntactically Awesome Stylesheets</a> - an extension of CSS3, adding nested rules, variables, mixins, selector inheritance, and more.</li>
<li><a href="http://garann.github.com/template-chooser/">Javascript Template Engine Chooser</a></li>
<li><a href="http://d3js.org">d3 data driven documents</a> - for programmatically creating html from data</li>
</ul>


<h2>Version Management</h2>

<p>Version management is vital for tracking progress, making experimental branches in your work, and coordinating across teams. The clear leader in this regard is <a href="http://git-scm.com">git</a> and the amazing project hosting environment <a href="https://github.com/">github</a>. I've got in the habit of versioning everything - not just my code - but my home file configurations too.</p>

<h2>And the rest...</h2>

<p>This is just a quick list I wrote off the top of my head. What have I missed? What else do you use?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Information Failure is not a Business Case]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/01/16/information-failure-is-not-a-business-case/"/>
    <updated>2013-01-16T14:39:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/01/16/information-failure-is-not-a-business-case</id>
    <content type="html"><![CDATA[<p>Ignorance is no excuse.</p>

<p>Here I'm referring to a public sector business case - i.e. a rationale for intervention in the economy based upon market failure. Not a business model. Information failure may indeed be <a href="/news/blog/2012/11/13/open-for-business/">a valid business model for adding value in the private sector</a>.</p>

<!--more-->


<p>It's all too common to see ignorance being used as a rationale for a public project - "if only more people knew about X then society/ the economy would be improved". If there are gains to trade available through better knowlege, then there are (rational) incentives for private individuals to learn. If X is a public sector project then this is actually marketing - justifiable on strategic grounds but not on the basis on market failure. If X is an intrinsically or publicly desirable course of action (e.g. not smoking) then the relevant market failure is actually the <strong>merit good</strong> case which is based on people having accurate information but choosing to ignore it - here again, information provision is actually a type of marketing.</p>

<p>The business case for information-based interventions should actually be based upon a different type of market failure: <strong>assymetric information</strong>. This occurs when either the buyer or seller knows more about the trade than the other. This can take the form of hidden information (you don't know the used-car salesman is selling you a lemon) or hidden action (your insurance company doesn't know whether you lock your door/ set the alarm every time you go out).  This imbalances leads to <strong>adverse selection</strong> (in the case of hidden information) or <strong>moral hazard</strong> (in the case of hidden action). Either way, the good quality products or services cannot be distinguished from the bad quality ones on the basis of price (the mechanism through which all markets operate).</p>

<p>Assymetric information is not simply solved by information provision. This market failure creates a business case for redressing the imbalance in information. It might include the creation of accreditation schemes or quality standards that, if credible, will buyers or sellers signal their quality or screen the quality of their trading partners.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[3 Metrics for Heath Economics Analysis]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/19/3-metrics-for-heath-economics-analysis/"/>
    <updated>2012-12-19T16:43:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/19/3-metrics-for-heath-economics-analysis</id>
    <content type="html"><![CDATA[<p>I'm often asked to suggest measures for assessing health interventions from an economic perspective. Here are explanations of four commonly used analytical methods:</p>

<ul>
<li>Reduction in Costs of Treatment</li>
<li>Cost per Quality Adjusted Life Years (QALY)</li>
<li>Incremental Cost Effectiveness Ratio (ICER)</li>
</ul>


<!--more-->


<p>As should be obvious from the titles, cost plays a significance role in health economics analysis. The first measure considers cost directly. The second measures also incorporate utility or benefit. The final measure should be used for interpretation.</p>

<p>In practice, it's advisable to consider using more than one method to account for the limitations of individual measures. It's also important to control for demographic and socio-economic characteristics such as age, gender, ethnicity, employment, and health condition. It will also be insightful to consider location as a proxy indicator for a range of (highly-correlated) socio-economic characteristics.</p>

<h2>Reduction in costs of treatment</h2>

<p>This is the most easily understandable economic impact of health interventions. If preventive steps are taken, fewer people contract disease, and less will need to be spent on treatment. The main focus of research under this measure is in <strong>quantifying the intervention's impact on health and the cost of treatment that is obviated</strong>. Weight loss, for example, will reduce the relative risk of diabetes and cardio-vascual disease. The relationship between intervention and disease risk might be established by reference to physiological measures (such as Body Mass Index). Cost of treatment might include the cost of prescription medicine, GP consultation, surgery and palliative care.</p>

<p>The difficulty with this sort of measure is that the impact is stated in terms of money that would otherwise have had to be spent and, as such, is quite intangigble.</p>

<h2>Cost per QALY</h2>

<p>A quality adjusted-life year, or QALY, represents <strong>the number of years of life that would be added by an intervention, taking into account the quality of those years</strong>. If the extra years would not be lived in full health they are given a value less than 1. Clearly the method of adjustment for quality is critical to this metric.
A commonly adopted measure is the <a href="http://www.euroqol.org/">EQ-5D</a> scale which seeks to categorise health states according to 5 dimensions: mobility, self-care, usual activities, pain/ discomfort and anxiety/ depression. This is combined with a quantiative measure of the patients self-assessment of their health on a <em>vertical analogue scale</em>.
Other approaches include the <em>time trade-off</em>, where respondents are asked to choose between remaining in an ill state or being restored to perfect health with a shorter life expectancy, and <em>standard gamble</em> where the alternative could restore them to perfect health or kill them.
This measure is criticised due to the difficulty of establishing a meaningful, objective, and comparable definition of "perfect health" and "disease burden".</p>

<h2>Incremental Cost-Effectiveness Ratio</h2>

<p>This ratio is used for comparing intervention options on a like for like basis. The Incremental Cost Effectiveness Ratio (ICER) is a <strong>comparison of the relative cost per QALY of the intervention and a reference case</strong> as follows (where QALY refers to a Quality Adjusted Life Year):
$$
ICER = \frac{ \text{£ Cost of intervention} - \text{£ Cost of reference case} } { \text{QALY Effect of intervention} –  \text{QALY Effect of reference case} }
$$
The ICER assumes a multiplicative model for QALYs (i.e. a change in scale will change the ICER ratio even though original figures remain the same).
Benchmarking against other interventions provides valuable contextual information. The <a href="https://research.tufts-nemc.org/cear4/default.aspx">Tufts Cost-Effectiveness Analysis Registry</a> is a valuable resource in this regard.</p>

<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Modest Evaluation and the Burden of Proof]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/11/evaluation-and-the-burden-of-proof/"/>
    <updated>2012-12-11T16:04:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/11/evaluation-and-the-burden-of-proof</id>
    <content type="html"><![CDATA[<p>Impact evaluation studies are often called upon to put a price on something that is impossible to value. Evaluators ought to take this limitation seriously when designing their research methods. This post discusses the problem and suggests some responses.</p>

<!-- more -->


<p>The <strong>burden of proof</strong> is said to lie with the party making a claim. In order to convince someone of some truth, you are obliged to support your claim with evidence. The audience hearing your claim will make a judgement by scrutinising the evidence you present in support.</p>

<p>This obligation should not be treated lightly. In some cases, it isn't possible to marshall evidence to support our intuitions. Indeed we can't always know what we don't know and so we shouldn't act like we can. Recognition of this limitation is known as <strong>epistemological modesty</strong>. This perspective doesn't mean that we have to admit defeat. Instead it is basis for action. Action that takes place with an awareness of our necessarily limited understanding.</p>

<p>This approach has important lessons for evaluation. It suggests that we should avoid the temptation to reduce the nuances of impact evaluation to a single result (i.e. an absolute figure in pounds sterling for the overall impact or a summary cost-benefit ratio). Although it might seem appealing to be able to boast about a large absolute impact figure or to point to a high return on investment, the evidence required to support those definite figures may not be credible. We should not invite the audience to question and undermine the claims being made. Instead we ought to make modest claims that allow for the greatest degree of flexibility over the evidence. <strong>It's better to be confident in a modest conclusion than unconfident in a bold one</strong>. Our efforts should be focussed on validating our assumptions and understanding situations under which our conclusions might be undermined. Thus the burden of proof is lightened and the claims more credibile.</p>

<p>Our ability to do this will, of course, depend upon the circumstance but examples of how this might work in practice include:</p>

<ul>
<li>establishing the minimum level of evidence required to demonstrate that benefits are at least equal to costs (rather than straining evidence to calculate the maximum plausible ratio of benefits to costs),</li>
<li>comparing projects on the basis of the credibility of the evidence required to demonstrate that break-even position,</li>
<li>deriving relative rather than absolute measures (so that we be certain that the comparisons are fair and reasonable,)</li>
<li>using confidence-intervals rather than point-estimates to make inferences, and</li>
<li>using evaluation to guide research (to identify otherwise implicit assumptions that need to be validated).</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is Infonomics?]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/06/what-is-infonomics/"/>
    <updated>2012-12-06T12:45:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/06/what-is-infonomics</id>
    <content type="html"><![CDATA[<p>When I started my <a href="http://infonomics.ltd.uk">economic consultancy and information analysis business</a> back in 2007 I was searching for a name that would describe my work in economics and information analysis and that was unique enough to appear early on in the search enginge result pages without too much trouble. At that time is was still cool to use a portmanteau (this was before dropping out a vowel caught-on) so I was certain that I was on to a winner.</p>

<!--more-->


<p>I didn't realise for some time that the term had already been coined by Doug Laney, then at META Group, to describe the practice of treating information as an asset. As an intangible asset information is given a price, appears on financial accounts, and is managed accordingly.</p>

<p>Since this definition now seems to dominate the <a href="http://en.wikipedia.org/w/index.php?title=Infonomics">wikipedia page</a> on the subject, I think it's about time I added my voice to the debate.</p>

<p>In my mind, at least, <strong>Infonomics is about information-based on economics, not economics-based on information</strong>. Whereas econometrics refers to economic statistics (and, let's be honest, principally regression analysis), I imagined Infonomics as a super-set including elements of computer science (such as information theory, algorithmic design, artificial intelligence, and data visualisation).</p>

<p>Of course there's rarely a single, unambigious definition of a neologism. But at least we can all agree that <a href="http://en.wikipedia.org/w/index.php?title=Infonomics&amp;diff=prev&amp;oldid=471082260">wikipedia should no longer say</a> "nobody has as yet referred to himself/herself as an infonomist".</p>
]]></content>
  </entry>
  
</feed>
