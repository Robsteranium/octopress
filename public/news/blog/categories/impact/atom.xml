<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: impact | Infonomics Blog]]></title>
  <link href="http://infonomics.ltd.uk/blog/blog/categories/impact/atom.xml" rel="self"/>
  <link href="http://infonomics.ltd.uk/blog/"/>
  <updated>2012-12-11T18:01:14+00:00</updated>
  <id>http://infonomics.ltd.uk/blog/</id>
  <author>
    <name><![CDATA[Robin Gower]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Modest Evaluation and the Burden of Proof]]></title>
    <link href="http://infonomics.ltd.uk/blog/blog/2012/12/11/evaluation-and-the-burden-of-proof/"/>
    <updated>2012-12-11T16:04:00+00:00</updated>
    <id>http://infonomics.ltd.uk/blog/blog/2012/12/11/evaluation-and-the-burden-of-proof</id>
    <content type="html"><![CDATA[<p>Impact evaluation studies are often called upon to put a price on something that is impossible to value. Evaluators ought to take this limitation seriously when designing their research methods. This post discusses the problem and suggests some responses.</p>

<!-- more -->


<p>The <strong>burden of proof</strong> is said to lie with the party making a claim. In order to convince someone of some truth, you are obliged to support your claim with evidence. The audience hearing your claim will make a judgement by scrutinising the evidence you present in support.</p>

<p>This obligation should not be treated lightly. In some cases, it isn't possible to marshall evidence to support our intuitions. Indeed we can't always know what we don't know and so we shouldn't act like we can. Recognition of this limitation is known as <strong>epistemological modesty</strong>. This perspective doesn't mean that we have to admit defeat. Instead it is basis for action. Action that takes place with an awareness of our necessarily limited understanding.</p>

<p>This approach has important lessons for evaluation. It suggests that we should avoid the temptation to reduce the nuances of impact evaluation to a single result (i.e. an absolute figure in pounds sterling for the overall impact or a summary cost-benefit ratio). Although it might seem appealing to be able to boast about a large absolute impact figure or to point to a high return on investment, the evidence required to support those definite figures may not be credible. We should not invite the audience to question and undermine the claims being made. Instead we ought to make modest claims that allow for the greatest degree of flexibility over the evidence. <strong>It's better to be confident in a modest conclusion than unconfident in a bold one</strong>. Our efforts should be focussed on validating our assumptions and understanding situations under which our conclusions might be undermined. Thus the burden of proof is lightened and the claims more credibile.</p>

<p>Our ability to do this will, of course, depend upon the circumstance but examples of how this might work in practice include:</p>

<ul>
<li>establishing the minimum level of evidence required to demonstrate that benefits are at least equal to costs (rather than straining evidence to calculate the maximum plausible ratio of benefits to costs),</li>
<li>comparing projects on the basis of the credibility of the evidence required to demonstrate that break-even position,</li>
<li>deriving relative rather than absolute measures (so that we be certain that the comparisons are fair and reasonable,)</li>
<li>using confidence-intervals rather than point-estimates to make inferences, and</li>
<li>using evaluation to guide research (to identify otherwise implicit assumptions that need to be validated).</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[5 Questions to Consider Before Commissioning an Evaluation]]></title>
    <link href="http://infonomics.ltd.uk/blog/blog/2012/12/05/questions-to-consider-before-commissioning-an-evaluation/"/>
    <updated>2012-12-05T10:50:00+00:00</updated>
    <id>http://infonomics.ltd.uk/blog/blog/2012/12/05/questions-to-consider-before-commissioning-an-evaluation</id>
    <content type="html"><![CDATA[<p>Planning is fundamental to successful evaluation. Considering these questions will help you to understand what you need to discuss with a contractor so that they can design an evaluation effectively.</p>

<!-- more -->


<h2>What is the objective?</h2>

<p>This ought to be defined in terms of the key questions that the evaluation ought to answer. The choice of question will determine which methods are used and what data is gathered.
Typical objectives include wanting to:</p>

<ul>
<li>have evidence to attract funding or investment</li>
<li>identify what has worked and what hasn't to make improvements</li>
<li>share lessons learned with partners</li>
<li>build plan for future activity</li>
</ul>


<p>Clearly a different approach to evaluation would be required to answer each of these questions.</p>

<p>In order to define the key evaluation question(s) it might be useful to consider:</p>

<ul>
<li>Who is the audience is for the evaluation (your board of directors, external funders, or participants)?</li>
<li>What will you do with the evaluation once it's complete?</li>
<li>What decisions will the evaluation help you make?</li>
</ul>


<h2>Who are your stakeholders or beneficiaries?</h2>

<p>Different people will value your work for different reasons. Those reasons won't always overlap and may even conflict. The evaluation might reach a different conclusion based upon the perspective you take. Answering this question will help you to determine scope and focus - indeed it's sometimes useful to think about the maximum possible range of beneficiaries just so that you can determine those whom won't be considered.</p>

<p>Thinking about stakeholders will also help with the preceding question. Often the stakeholder may also be a funder. It's important in these cases to have the relevant people involved in the evaluation from the outset. This means discussing the objectives and proposed methods with them. Not only will this give your conclusions greater credibility with the target audience, but it will also help to ensure relevance. Further, they might be able to offer insight or resources from other work they've seen.</p>

<h2>What can we do internally, where do we need support?</h2>

<p>You can expect good evaluation to cost at least 10% of the operating costs of the service being evaluated. It not necessary to outsource all of the evaluation work. Even where you are contractually obliged to appoint an independent consultant it is still possible for you to be involved.</p>

<p>External support will typically be need for technical aspects such as:</p>

<ul>
<li>establish a research methodology or epsitemological framework for answering your key evaluation questions</li>
<li>designing primary research (sampling, questionnaires, or topic guides)</li>
<li>devising and analysing statistical models to establish behavioural change, causal effects or to financial proxies for non-traded costs and benefits</li>
</ul>


<p>Whereas internal work might focus on activities like collecting monitoring data.</p>

<p>As discussed above, it is vital that internal staff are involved in the conversations with stakeholders if the evaluation is going to be useful in future.</p>

<h2>Do we understand the evaluation design?</h2>

<p>In order to interpret the results of the evaluation it will be vital for you to understand how those results were reached. This doesn't necessarily mean, for example, that you need to know how to calculate a "two-stage Heckit regression". You do at least need to know why this method was used (i.e. in response to selection bias) so that you know that the findings do account for the fact that sample of people consulted was biased. This is important for forming your own judgements about the results and for dealing with criticisms from others.</p>

<p>A empirically flawless method is useless if none of your audience can make sense of the results. That's not to say that you should avoid technical research methods; indeed this question might be better phrased as: what do we need to learn to understand the evaluation design?</p>

<h2>What data do we already have?</h2>

<p>Having data available may help to save on the costs of data collection. Data can come from obvious sources such as:</p>

<ul>
<li>the monitoring data your contractually obliged to collect,</li>
<li>management information you use to direct your work, or</li>
<li>feedback forms you've used to ongoing service development.</li>
</ul>


<p>Data can also come from unexpected places like:</p>

<ul>
<li>administrative databases,</li>
<li>server access records or log files, and</li>
<li>email address books or messages.</li>
</ul>


<p>You should also expect to provide evaluators with:</p>

<ul>
<li>contact information for consultees - at the strategic and beneficiary level,</li>
<li>background information (particularly any work done to scope, appraise, design or commission your service), and</li>
<li>contractual and financial records.</li>
</ul>

]]></content>
  </entry>
  
</feed>
