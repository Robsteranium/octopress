<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: impact | Infonomics Blog]]></title>
  <link href="http://infonomics.ltd.uk/news/blog/categories/impact/atom.xml" rel="self"/>
  <link href="http://infonomics.ltd.uk/news/"/>
  <updated>2013-01-16T15:09:11+00:00</updated>
  <id>http://infonomics.ltd.uk/news/</id>
  <author>
    <name><![CDATA[Robin Gower]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[3 Metrics for Heath Economics Analysis]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/19/3-metrics-for-heath-economics-analysis/"/>
    <updated>2012-12-19T16:43:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/19/3-metrics-for-heath-economics-analysis</id>
    <content type="html"><![CDATA[<p>I'm often asked to suggest measures for assessing health interventions from an economic perspective. Here are explanations of four commonly used analytical methods:</p>

<ul>
<li>Reduction in Costs of Treatment</li>
<li>Cost per Quality Adjusted Life Years (QALY)</li>
<li>Incremental Cost Effectiveness Ratio (ICER)</li>
</ul>


<!--more-->


<p>As should be obvious from the titles, cost plays a significance role in health economics analysis. The first measure considers cost directly. The second measures also incorporate utility or benefit. The final measure should be used for interpretation.</p>

<p>In practice, it's advisable to consider using more than one method to account for the limitations of individual measures. It's also important to control for demographic and socio-economic characteristics such as age, gender, ethnicity, employment, and health condition. It will also be insightful to consider location as a proxy indicator for a range of (highly-correlated) socio-economic characteristics.</p>

<h2>Reduction in costs of treatment</h2>

<p>This is the most easily understandable economic impact of health interventions. If preventive steps are taken, fewer people contract disease, and less will need to be spent on treatment. The main focus of research under this measure is in <strong>quantifying the intervention's impact on health and the cost of treatment that is obviated</strong>. Weight loss, for example, will reduce the relative risk of diabetes and cardio-vascual disease. The relationship between intervention and disease risk might be established by reference to physiological measures (such as Body Mass Index). Cost of treatment might include the cost of prescription medicine, GP consultation, surgery and palliative care.</p>

<p>The difficulty with this sort of measure is that the impact is stated in terms of money that would otherwise have had to be spent and, as such, is quite intangigble.</p>

<h2>Cost per QALY</h2>

<p>A quality adjusted-life year, or QALY, represents <strong>the number of years of life that would be added by an intervention, taking into account the quality of those years</strong>. If the extra years would not be lived in full health they are given a value less than 1. Clearly the method of adjustment for quality is critical to this metric.
A commonly adopted measure is the <a href="http://www.euroqol.org/">EQ-5D</a> scale which seeks to categorise health states according to 5 dimensions: mobility, self-care, usual activities, pain/ discomfort and anxiety/ depression. This is combined with a quantiative measure of the patients self-assessment of their health on a <em>vertical analogue scale</em>.
Other approaches include the <em>time trade-off</em>, where respondents are asked to choose between remaining in an ill state or being restored to perfect health with a shorter life expectancy, and <em>standard gamble</em> where the alternative could restore them to perfect health or kill them.
This measure is criticised due to the difficulty of establishing a meaningful, objective, and comparable definition of "perfect health" and "disease burden".</p>

<h2>Incremental Cost-Effectiveness Ratio</h2>

<p>This ratio is used for comparing intervention options on a like for like basis. The Incremental Cost Effectiveness Ratio (ICER) is a <strong>comparison of the relative cost per QALY of the intervention and a reference case</strong> as follows (where QALY refers to a Quality Adjusted Life Year):
$$
ICER = \frac{ \text{£ Cost of intervention} - \text{£ Cost of reference case} } { \text{QALY Effect of intervention} –  \text{QALY Effect of reference case} }
$$
The ICER assumes a multiplicative model for QALYs (i.e. a change in scale will change the ICER ratio even though original figures remain the same).
Benchmarking against other interventions provides valuable contextual information. The <a href="https://research.tufts-nemc.org/cear4/default.aspx">Tufts Cost-Effectiveness Analysis Registry</a> is a valuable resource in this regard.</p>

<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Modest Evaluation and the Burden of Proof]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/11/evaluation-and-the-burden-of-proof/"/>
    <updated>2012-12-11T16:04:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/11/evaluation-and-the-burden-of-proof</id>
    <content type="html"><![CDATA[<p>Impact evaluation studies are often called upon to put a price on something that is impossible to value. Evaluators ought to take this limitation seriously when designing their research methods. This post discusses the problem and suggests some responses.</p>

<!-- more -->


<p>The <strong>burden of proof</strong> is said to lie with the party making a claim. In order to convince someone of some truth, you are obliged to support your claim with evidence. The audience hearing your claim will make a judgement by scrutinising the evidence you present in support.</p>

<p>This obligation should not be treated lightly. In some cases, it isn't possible to marshall evidence to support our intuitions. Indeed we can't always know what we don't know and so we shouldn't act like we can. Recognition of this limitation is known as <strong>epistemological modesty</strong>. This perspective doesn't mean that we have to admit defeat. Instead it is basis for action. Action that takes place with an awareness of our necessarily limited understanding.</p>

<p>This approach has important lessons for evaluation. It suggests that we should avoid the temptation to reduce the nuances of impact evaluation to a single result (i.e. an absolute figure in pounds sterling for the overall impact or a summary cost-benefit ratio). Although it might seem appealing to be able to boast about a large absolute impact figure or to point to a high return on investment, the evidence required to support those definite figures may not be credible. We should not invite the audience to question and undermine the claims being made. Instead we ought to make modest claims that allow for the greatest degree of flexibility over the evidence. <strong>It's better to be confident in a modest conclusion than unconfident in a bold one</strong>. Our efforts should be focussed on validating our assumptions and understanding situations under which our conclusions might be undermined. Thus the burden of proof is lightened and the claims more credibile.</p>

<p>Our ability to do this will, of course, depend upon the circumstance but examples of how this might work in practice include:</p>

<ul>
<li>establishing the minimum level of evidence required to demonstrate that benefits are at least equal to costs (rather than straining evidence to calculate the maximum plausible ratio of benefits to costs),</li>
<li>comparing projects on the basis of the credibility of the evidence required to demonstrate that break-even position,</li>
<li>deriving relative rather than absolute measures (so that we be certain that the comparisons are fair and reasonable,)</li>
<li>using confidence-intervals rather than point-estimates to make inferences, and</li>
<li>using evaluation to guide research (to identify otherwise implicit assumptions that need to be validated).</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[5 Questions to Consider Before Commissioning an Evaluation]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/05/questions-to-consider-before-commissioning-an-evaluation/"/>
    <updated>2012-12-05T10:50:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/05/questions-to-consider-before-commissioning-an-evaluation</id>
    <content type="html"><![CDATA[<p>Planning is fundamental to successful evaluation. Considering these questions will help you to understand what you need to discuss with a contractor so that they can design an evaluation effectively.</p>

<!-- more -->


<h2>What is the objective?</h2>

<p>This ought to be defined in terms of the key questions that the evaluation ought to answer. The choice of question will determine which methods are used and what data is gathered.
Typical objectives include wanting to:</p>

<ul>
<li>have evidence to attract funding or investment</li>
<li>identify what has worked and what hasn't to make improvements</li>
<li>share lessons learned with partners</li>
<li>build plan for future activity</li>
</ul>


<p>Clearly a different approach to evaluation would be required to answer each of these questions.</p>

<p>In order to define the key evaluation question(s) it might be useful to consider:</p>

<ul>
<li>Who is the audience is for the evaluation (your board of directors, external funders, or participants)?</li>
<li>What will you do with the evaluation once it's complete?</li>
<li>What decisions will the evaluation help you make?</li>
</ul>


<h2>Who are your stakeholders or beneficiaries?</h2>

<p>Different people will value your work for different reasons. Those reasons won't always overlap and may even conflict. The evaluation might reach a different conclusion based upon the perspective you take. Answering this question will help you to determine scope and focus - indeed it's sometimes useful to think about the maximum possible range of beneficiaries just so that you can determine those whom won't be considered.</p>

<p>Thinking about stakeholders will also help with the preceding question. Often the stakeholder may also be a funder. It's important in these cases to have the relevant people involved in the evaluation from the outset. This means discussing the objectives and proposed methods with them. Not only will this give your conclusions greater credibility with the target audience, but it will also help to ensure relevance. Further, they might be able to offer insight or resources from other work they've seen.</p>

<h2>What can we do internally, where do we need support?</h2>

<p>You can expect good evaluation to cost at least 10% of the operating costs of the service being evaluated. It not necessary to outsource all of the evaluation work. Even where you are contractually obliged to appoint an independent consultant it is still possible for you to be involved.</p>

<p>External support will typically be need for technical aspects such as:</p>

<ul>
<li>establish a research methodology or epsitemological framework for answering your key evaluation questions</li>
<li>designing primary research (sampling, questionnaires, or topic guides)</li>
<li>devising and analysing statistical models to establish behavioural change, causal effects or to financial proxies for non-traded costs and benefits</li>
</ul>


<p>Whereas internal work might focus on activities like collecting monitoring data.</p>

<p>As discussed above, it is vital that internal staff are involved in the conversations with stakeholders if the evaluation is going to be useful in future.</p>

<h2>Do we understand the evaluation design?</h2>

<p>In order to interpret the results of the evaluation it will be vital for you to understand how those results were reached. This doesn't necessarily mean, for example, that you need to know how to calculate a "two-stage Heckit regression". You do at least need to know why this method was used (i.e. in response to selection bias) so that you know that the findings do account for the fact that sample of people consulted was biased. This is important for forming your own judgements about the results and for dealing with criticisms from others.</p>

<p>A empirically flawless method is useless if none of your audience can make sense of the results. That's not to say that you should avoid technical research methods; indeed this question might be better phrased as: what do we need to learn to understand the evaluation design?</p>

<h2>What data do we already have?</h2>

<p>Having data available may help to save on the costs of data collection. Data can come from obvious sources such as:</p>

<ul>
<li>the monitoring data your contractually obliged to collect,</li>
<li>management information you use to direct your work, or</li>
<li>feedback forms you've used to ongoing service development.</li>
</ul>


<p>Data can also come from unexpected places like:</p>

<ul>
<li>administrative databases,</li>
<li>server access records or log files, and</li>
<li>email address books or messages.</li>
</ul>


<p>You should also expect to provide evaluators with:</p>

<ul>
<li>contact information for consultees - at the strategic and beneficiary level,</li>
<li>background information (particularly any work done to scope, appraise, design or commission your service), and</li>
<li>contractual and financial records.</li>
</ul>

]]></content>
  </entry>
  
</feed>
