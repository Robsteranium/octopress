<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Infonomics Blog]]></title>
  <link href="http://infonomics.ltd.uk/news/atom.xml" rel="self"/>
  <link href="http://infonomics.ltd.uk/news/"/>
  <updated>2015-02-20T17:22:06+00:00</updated>
  <id>http://infonomics.ltd.uk/news/</id>
  <author>
    <name><![CDATA[Robin Gower]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The Linked Data Mind Set]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2015/02/20/the-linked-data-mind-set/"/>
    <updated>2015-02-20T17:13:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2015/02/20/the-linked-data-mind-set</id>
    <content type="html"><![CDATA[<p>Linked Data is data that has been structured and published in such a way that it may be interlinked as part of the Semantic Web. In contrast to the traditional web, which is aimed at human readers, the semantic web is designed to be machine readable. It is built upon standard web technologies &ndash; <acronym title="Hypertext Transfer Protocol">HTTP</acronym>, <acronym title="Resource Description Framework">RDF<acronym>, and <acronym title="Uniform Resource Identifier">URI</acronym>s.</p>

<p>I&rsquo;ve been working with Manchester-based Linked Data pioneers <a href="http://www.swirrl.com/">Swirrl</a> to convert open data to linked data format. This experience has opened my eyes to the immense power of linked data. I thought it was simply a good, extensible structure with some nice web-oriented features. What I&rsquo;ve actually found is some pretty fundamental differences that require quite a change in mind set.</p>

<!--more-->


<h2>Introduction to Linked Data</h2>

<p>If you&rsquo;re already familiar with linked-data then jump down to read about the changes in perspective it&rsquo;s led me to see. If you&rsquo;re new to the topic or a bit rusty then you might want to read about the basic principles first.</p>

<p>The recently updated <a href="http://www.w3.org/TR/2014/NOTE-rdf11-primer-20140225/">RDF Primer 1.1</a> provides an excellent introduction to RDF. A brief summary follows.</p>

<h3>Everything is a graph</h3>

<p>Graphs, in the mathematical sense, are collections of nodes joined by edges. In linked-data this is described in terms of triples &ndash; statements which relate a subject to an object via a predicate:</p>

<pre><code>&lt;subject&gt; &lt;predicate&gt; &lt;object&gt;
&lt;Bob&gt; &lt;is a&gt; &lt;person&gt;
&lt;Bob&gt; &lt;is a friend of&gt; &lt;Alice&gt;
&lt;Bob&gt; &lt;is born on&gt; &lt;the 4th of July 1990&gt;
</code></pre>

<p>These statements are typically grouped together into graphs or contexts. A quad statement has a subject, predicate, object, and context (or graph).</p>

<h3>URIs and Literal</h3>

<p>The subjects and predicates are all identifiers symbolic representations that a supposed to be globally unique, called uniform resource identifiers (URIs). URIs are much like URLs (Uniform Resource Locators) that you may be familiar with using to find web pages (this &ldquo;finding&rdquo; process &ndash; requesting a URL in your browser to get a web page in response &ndash; is more technically known as &ldquo;dereferencing&rdquo;). URIs are a superset of URLs which also include URNs (Uniform Resource Names) such as ISBNs (International Standard Book Numbers).</p>

<p>The objects can also be URIs or they can take the form of literal values (like strings, numbers and dates).</p>

<h3>Turtle and SPARQL</h3>

<p>There are a number of serialisation formats for RDF. By far the most readable is <a href="http://www.w3.org/TR/turtle/">Turtle</a>.</p>

<pre><code>BASE   &lt;http://example.org/&gt;
PREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;
PREFIX xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt;
PREFIX schema: &lt;http://schema.org/&gt;

&lt;bob#me&gt;
    a foaf:Person ;
    foaf:knows &lt;alice#me&gt; ;
    schema:birthDate "1990-07-04"^^xsd:date ;
</code></pre>

<p><a href="http://www.w3.org/TR/rdf-sparql-query/">SPAQRL</a> is a query language for RDF. The query below selects Bob.</p>

<pre><code>SELECT ?person WHERE { ?person foaf:knows &lt;http://example.org/alice#me&gt; }
</code></pre>

<h2>Thinking with a Linked Data Mindset</h2>

<p>Now that we&rsquo;ve established the basics, we can go on to consider how this perspective can lead to a different mindset.</p>

<h3>There&rsquo;s no distinction between data and metadata</h3>

<p>Metadata is data that describes data. For example, the date a dataset was published. In traditional spreadsheets there&rsquo;s not always an obvious place to put this information. It&rsquo;s recorded in the filename or on a &ldquo;miscellanous details&rdquo; sheet. This isn&rsquo;t ideal as a) it&rsquo;s not generally referenceable, and b) it is easily lost if it&rsquo;s not copied around with the data itself.</p>

<p>In RDF, metadata is stored in essentially the same way as data. It&rsquo;s triples all the way down! Certainly there are some vocabularies that are designed for metadata purposes (<a href="http://dublincore.org/">Dublin Core</a>, <a href="http://www.w3.org/TR/void/">VOID</a>, etc) but the content is described using the same structures and is amenable to the same sorts of interrogation techniques.</p>

<p>This makes a lot of sense when you think about it. Metadata serves two purposes: to enable discovery and to allow the recording of facts that wouldn&rsquo;t otherwise fit.</p>

<p>Discovery is the process of finding data relevant to your interests. Metadata summarises the scope of a dataset so that we can make requests like: &ldquo;show me all of the datasets published since XXXX about YYYY available on a neighbourhood level&rdquo;. But this question could be answered with the data itself. The distinction between metadata and data exists in large part, because of the way we package data. That is to say we typically present data in spreadsheets where the content and scope cannot be accessed without the user first acquiring and then interpreting the data. Obviously this can&rsquo;t be done in bulk unless the spreadsheets follow a common schema (some human interaction is otherwise necessary to prepare the data). If we remove the data from these packages, and allow deep inspection of it&rsquo;s content, then discovery can be acheived without resorting to a separate metadata index (although metadata descriptions can still make the process more efficient).</p>

<p>The recording of facts that don&rsquo;t fit is usually a problem for metadata because it doesn&rsquo;t vary along the dimensions of the dataset in the traditional (tabular) way it&rsquo;s usually present. This isn&rsquo;t a problem for linked data.</p>

<h3>The entity-relationship model doesn&rsquo;t (always) fit</h3>

<p>The capacity of entity-relationship models is demonstrated by the popularity of object-oriented programming and relational-databases. Linked-data too can represent entity-relationship very naturally. The typically problem with the ER approach is that there&rsquo;s so often an exception to the rule. A given entity doesn&rsquo;t fit with the others and has a few odd properties that don&rsquo;t apply to everything else. Different relationships between instances of the same two types (typically recorded with primary/ foreign keys) are qualitatively different. Since in ER, information about an object is stored within it, the data model can become brittle. In linked-data, properties can be defined quite apart from objects.</p>

<h3>There&rsquo;s no schema: arbitrary data can be added anywhere</h3>

<p>In a traditional table representation, it&rsquo;s awkward to add arbitrary data. If you want to add a datum that doesn&rsquo;t fit into the schema then the schema must be modified. Adding new columns for a single datum is wasteful, and quickly leads to a bloated and confusing list of seldom-used fields.</p>

<p>In part, this frustration gave rise to the Schemaless/ NoSQL databases. These systems sit at the other end of the scale. Without any structure it can be complex to make queries and maintain data integrity. These problems are shifted from the database to the application layer.</p>

<p>In a graph representation, anything can be added anywhere. The schema is in the data itself and we can decide how much structure (like constraints and datatypes) we want to add.</p>

<h3>The data is self-describing</h3>

<p>This flexibility &ndash; the ability to add arbitrary facts without the constriction of a schema &ndash; can certainly seem daunting. Without a schema what is going to prevent errors, provide guarantees, or ensure consistency? In fact linked-data does have a schema of sorts. Vocabularies are used to describe the data. A few popular ontologies are worth mentioning:</p>

<ul>
<li><a href="http://www.w3.org/TR/rdf-schema/">RDFS</a>: the RDF Schema extends the basic RDF vocabulary to include a class and property system.</li>
<li><a href="http://www.w3.org/TR/owl-primer/">OWL</a>: the Web Ontology Language is designed to represent rich and complex knowledge about things, groups of things, and relations between things</li>
<li><a href="http://www.w3.org/TR/skos-primer">SKOS</a>: provides a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary.</li>
</ul>


<h3>There&rsquo;s no one right way to do things</h3>

<p>The flexibility of the data format means that there are often several ways to model the same dataset. This can lead to a sort of options-paralysis! It often pays to make a choice for the sake of progress, then review it later once more of the pieces of the puzzle are in place. Realising that it doesn&rsquo;t need to be perfect first time is certainly liberating.</p>

<h3>Naming is hard</h3>

<p>Naming is one of the hardest problems in programming. Linked data modelling is 90% naming. The <a href="http://patterns.dataincubator.org/book/">Linked Data Patterns book</a> provides some useful suggestions for how to approach naming (URI design) in a range of contexts.</p>

<p><a href="http://thomsonreuters.com/corporate/pdf/creating-value-with-identifiers-in-an-open-data-world.pdf">Identifiers have value</a>: clarifying ambiguity, promoting consensus, providing reliability, ensuring stability, and facilitating integration.</p>

<h3>Vocabularies aren&rsquo;t settled</h3>

<p>When developing a linked-data model, it&rsquo;s vital to understand the work done by others before you. After all, you need to adopt other vocabularies and URIs in order to link your data to the rest of the semantic web. There are lots of alternatives. The <a href="http://lov.okfn.org">Linked Open Vocabularies</a> site provides a way to search and compare vocabularies to help you decide which to use.</p>

<h2>The Linked-Data Mind Set</h2>

<p>In summary:</p>

<ul>
<li>Metadata can be data too, don&rsquo;t treat it as a second class citizen</li>
<li>Use entities if it helps, but don&rsquo;t get too hung-up on them</li>
<li>Let your schema grow and change over time as you learn more about the domain</li>
<li>Use the core vocabularies to bring commonly understood structure to your data</li>
<li>Experiment with different models to see what works best for your data and applications</li>
<li>Create identifiers &ndash; it might be hard to start with, but everybody benefits in the long-term</li>
<li>Stand on the shoulders of giants &ndash; follow patterns and adopt vocabularies</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How Information Entropy teaches us to Improve Data Quality]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2014/05/08/how-information-entropy-teaches-us-to-improve-data-quality/"/>
    <updated>2014-05-08T10:02:00+01:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2014/05/08/how-information-entropy-teaches-us-to-improve-data-quality</id>
    <content type="html"><![CDATA[<p>I&rsquo;m often asked by data-owners for guidance on sharing data, whether it&rsquo;s with me on consulting engagements or by organisations looking to <a href="http://infonomics.ltd.uk/news/blog/2013/03/01/open-data-business-models/">release the potential</a> of their <a href="http://infonomics.ltd.uk/news/blog/categories/open-data/">open data</a>.</p>

<p>A great place to start is the <a href="http://5stardata.info/">5 star deployment scheme</a> which describes a maturity curve for open data:</p>

<ol>
<li> ★ make your stuff available on the Web (i.e. in whatever format) under an open license</li>
<li> ★★ make it available as structured machine-readable data (e.g. Excel instead of image scan of a table)</li>
<li> ★★★  use non-proprietary formats (e.g. CSV instead of Excel)</li>
<li> ★★★★   use <acronym title="Uniform Resource Identifier">URI</acronym>s to denote things, so that people can point at your stuff</li>
<li> ★★★★★    link your data to other data to provide context</li>
</ol>


<p>This scheme certainly provides a strategic overview (release early/ improve later, embrace openness, aim to create linked open data) but it doesn&rsquo;t say much about specific questions such as: how should the data be structured or presented and what should it include?</p>

<p>I have prepared the below advice based upon the experiences I&rsquo;ve had as a consumer of data, common obstacles to analysis that might have been avoided if the data had been prepared in the right way.</p>

<p>In writing this, it occurs to me that the general principle is to increase information entropy. <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Information entropy</a> is a measure of the expected value of a message. It is higher when that message (once delivered) is able to resolve more uncertainty. That is to say, that the message is able to say more things, more clearly, that are novel to the recipient.</p>

<!--more-->


<h2>More is usually better than less (but don&rsquo;t just repeat everybody else)</h2>

<p>While it is (comparatively) easy to ignore irrelevant or useless data, it is impossible to consider data that you don&rsquo;t have. If it&rsquo;s easy enough to share everything then do so. Bandwidth is cheap and it&rsquo;s relatively straightforward to filter data. Those analysing your data may have a different perspective on what&rsquo;s useful &ndash; you don&rsquo;t know what they don&rsquo;t know.</p>

<p>This may be inefficient, particularly if the receiver is already in possession of the data you&rsquo;re sending. Where your data set includes data from a third party it may be better to provide a linking index to that data, rather than to replicate it wholesale. Indeed even if the data you have available to release is small, it may be made larger through linking it to other sources.</p>

<h2>Codes and Codelists allow for linking (which makes your data more valuable)</h2>

<p>There are positive network effects to data linking &ndash; the value of data grows exponentially as not only may it be linked with other data, but that other data may be linked with it. Indeed, perhaps the most valuable data sources of all are the indicies that allow for linking between datasets. This is often called reference data &ndash; sets of permissible values that ensure that two datasets refer to a common concept in the same terms. The quality of a dataset may be improved by adding reference data or codes from standard code lists. A typical example of this is the Government Statistical Service codes that the ONS use to identify geographic areas in the UK (this is much prefered over area names that can&rsquo;t be linked because of differences in spelling that prevent &ndash; &ldquo;Bristol&rdquo; or &ldquo;Bristol, City of&rdquo;, it&rsquo;s all <a href="http://statistics.data.gov.uk/explore?URI=http://statistics.data.gov.uk/id/statistical-geography/E06000023">E06000023</a> to me!).</p>

<p>If you&rsquo;re creating your own codelist it ought to follow the C.E.M.E. principle &ndash; Comprehensively Exhaustive and Mutually Exclusive. If the codes don&rsquo;t cover a significant category you&rsquo;ll have lot&rsquo;s of &ldquo;other&#8221;s which will basically render the codelist useless. If the codes overlap then they can&rsquo;t be compared and the offending codes will ultimately need to be combined.</p>

<h2>Normalised data is more reliable and more efficient</h2>

<p>Here I&rsquo;m referring to <a href="http://en.wikipedia.org/wiki/Database_normalization">database normalisation</a>, rather than <a href="http://en.wikipedia.org/wiki/Normalization_%28statistics%29">statistical normalisation</a>. A normalised database is one with a minimum redundancy &ndash; the same data isn&rsquo;t repeated in multiple places. Look-up tables are used, for example, so that a categorical variable doesn&rsquo;t need to have it&rsquo;s categories repeated (and possibly misspelled). If you have a table with two or more rows that need to be changed at the same time (because in some place they&rsquo;re referring to the same thing) then some normalisation is required.</p>

<p>Database normalisation ensures integrity (otherwise if two things purporting to be the same are different then how do you know which one is right?) and efficiency (repetition is waste).</p>

<h2>Be precise, allow data users to simplify (as unsimplification isn&rsquo;t possible)</h2>

<p>Be wary about introducing codes where they&rsquo;re unneccessary. It&rsquo;s unfortunately quite common to see a continuous variable represented by categories. This seems to be particularly common with Age. The problem is, of course, that different datasets make different choices about the age intervals, and so can&rsquo;t be compared. One might use &lsquo;working age&rsquo; 16-74 and another &lsquo;adult&rsquo; 15+. Unless data with the original precision can be found, then the analyst will need to apportion or interpolate values in between categories.</p>

<p>Categories that do not divide a continuous dimension evenly are also problematic. This is particularly common in survey data, where respondents are presented with a closed-list of intervals as options, rather than being asked to provide an estimate of the value itself. The result is often that the majority of responses fall into one category, with few in the others. Presenting a closed-list of options is sometimes to be prefered for other reasons (e.g. in questions about income, categories might ellicit more responses) &ndash; if so the bounds should be chosen with reference to the expected frequencies of responses not the linear scale of the dimension (i.e. the categories should have similar numbers of observations in them, not occupy similar sized intervals along the range of the variable being categorised).</p>

<p>Precise data can be codified into less precise data. The reverse process is not possible (or at least not accurately).</p>

<h2>Represent Nothingness accurately (be clear even when you don&rsquo;t know)</h2>

<p>It&rsquo;s important to distinguish between different types of nothingness. Nothing can be:</p>

<ul>
<li>Not available &ndash; where no value has been provided (the value is unknown);</li>
<li>Null &ndash; where the value is known to be nothing;</li>
<li>Zero &ndash; which is actually a specific number (although it may sometimes be used to represent null).</li>
</ul>


<p>A blank space or a number defaulting to 0 could be any of these types of nothingness. Not knowing which type of nothing you&rsquo;re dealing with can undermine analysis.</p>

<h2>Provide metadata (describe and explain your data)</h2>

<p>Metadata is data about data. It describes provenance (how the data was collected or derived) and coverage (e.g. years, places, limits to scope, criteria for categories), and provides warnings about assumptions and their implications for interpretation.</p>

<p>Metadata isn&rsquo;t just a descriptive narrative. It can be analysed as data itself. It can tell someone whether or not your data is relevant to their requirements without them having to download and review it.</p>

<h2>In summary &ndash; increase information entropy</h2>

<p>These tips are all related to a general principle of increasing entropy. As explained above, <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Information entropy</a> is a measure of the expected value of a message. It is higher when that message (once delivered) is able to resolve more uncertainty. That is to say, that the message is able to say more things, more clearly, that are novel to the recipient.</p>

<ul>
<li>More data, whether in the original release or in the other sources that may be linked to it, means more variety, which means more uncertainty can be resolved, and thus more value provided.</li>
<li>Duplication (and thus the potential for inconsistency) in the message means that it doesn&rsquo;t resolve uncertainty, and thus doesn&rsquo;t add value.</li>
<li>Normalised data retains the same variety in a smaller, clearer message.</li>
<li>Precise data can take on more possible values and thus clarify more uncertainty than codified data.</li>
<li>Inaccurately represented nothingness also means that the message isn&rsquo;t able to resolve uncertainty (about which type of nothing applies).</li>
<li>Metadata makes the recipient more certain about the content of your data</li>
</ul>


<p>Herein lies a counter-intuitive aspect of releasing data. It seems to be sensible to reduce variety and uncertainty in the data, to make sense and interpret the raw data before it is presented. To provide more rather than less ordered data. In fact such actions make the data less informative, and make it harder to re-interpret the data in a wider range of contexts. Indeed much of the impetus behind Big Data is the recognition that unstructured, raw data has immense information potential. It is the capacity for re-interpretation that makes data valuable.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sonification and Auditory Display Primer]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2014/01/19/sonification-and-auditory-display-primer/"/>
    <updated>2014-01-19T18:36:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2014/01/19/sonification-and-auditory-display-primer</id>
    <content type="html"><![CDATA[<p>An auditory display uses sound to convery information. Sonification is defined as a type auditory display that uses non-speech audio, rendering sound in response to data and interaction.</p>

<p>This post provides a brief introduction to sonfication based upon <a href="http://sonification.de/handbook/index.php/chapters/chapter2/">Chapter 2</a> of the <a href="http://sonification.de/handbook/">Sonification Handbook</a>.</p>

<div class="well">
  <p>A <a href="http://sonify.psych.gatech.edu/research/soundscapes/">soundscape mapping a stock market index to ecological sounds</a>. As the index rises, bird calls, crickets, frogs, and other forest sounds are added. As the index falls, rain and thunder are heard. The soundscape is designed to be monitoring peripherally and not to be intrusive.</p>
  <p><audio src="http://sonify.psych.gatech.edu/research/soundscapes/sounds/fullsample01.mp3" type="audio/mpeg" controls></audio></p>
</div>




<!--more-->


<h2>Why sonification is useful</h2>

<div class="well callout">
  <p><a href="http://silentlistening.wordpress.com/2008/05/09/dispersion-of-sound-waves-in-ice-sheets/">Andreas Bick&#8217;s field recording of ice cracks</a> is already in the audible range. The recording exhibits the difference between the transmission of the echoes through ice and water.</p>
  <p><audio src="http://silentlistening.files.wordpress.com/2008/04/frost-pattern.mp3" controls></audio></p>
</div>


<ul>
<li>the human auditory system is good at recognising temporal changes or patterns</li>
<li>the operator may not always be able to see a visual display

<ul>
<li>the content does not require constant observation such as warning alarms</li>
<li>an existing visual system may be overloaded</li>
<li>the perceiver may be visually impaired</li>
</ul>
</li>
<li>the data is verbal-categorical, has a high number of dimensions, or requires rapid detection</li>
<li>because the rise of mobile devices mean smaller screens and less room for visual displays</li>
</ul>


<h2>Use cases</h2>

<div class="well callout">
  <p>An auditory menu designed for use by drivers. You can hear the user scrolling through an address book. The spindex cues give an overview of alphabetical position, with slower navigation triggering spearcons for each contact.</p>
  <p><audio src="http://sonification.de/handbook/media/chapter2/SHB-S2.2.mp3" type="audio/mpeg" controls></audio></p>
</div>


<ul>
<li>alerts and notifications (indicating an occurance)</li>
<li>alarms and warnings (indicating an adverse occurance, perhaps requiring an urgent response)</li>
<li>status and progress indicators</li>
<li>data exploration and auditory graphs</li>
<li>art, entertainment, sport and leisure</li>
</ul>


<h2>Types of Sonification</h2>

<h3>Interaction</h3>

<div class="well callout">
<p>
<img src="http://infonomics.ltd.uk/news/images/post_images/tectonics.png" alt="Tectonics thumbnail" />
Florian Dombois&#8217; audification of <a href="http://www.auditory-seismology.org/version2004/tectonics.html">Plate Tectonics</a> demonstrates the difference between the plop of the parting Atlantic ocean plates and the crack of the plates drifting against each other).
</p>
</div>


<p>At one end of the scale, there are non-interactive sonifications that once triggered play in their entirety as a concert or a tour around the information. This has parallels with the direction instruction method of teaching whereby an existing conclusion or viewpoint is demonstrated.</p>

<p>At the other end of the scale, there are user-initiated sonifications that require the user to engage in a conversation. This has parallels with the enquiry-based learning method which begins with questions, problems, or scenarios allowing knowledge to be discovered through exploration.</p>

<p>Somewhere in between lies the facility for manipulation of the sonification at basic level &ndash; controlling the speed, pausing, fast-forwarding, and/ or rewinding.</p>

<h3>Methods</h3>

<ul>
<li><em>Audification</em> transforms periodic or other data that has a waveform structure into the audible range.</li>
<li><em>Parameter Mapping Sonification</em> extends this to other data forms, mapping a data dimension to an accoustic dimension</li>
<li><em>Model-based Sonfication</em> emerges from the interaction of a user with an instrument so that the data structure is understood through the sonic responses of a virtual object.</li>
</ul>


<p>An <em>auditory icon</em> represents something and bears an analogous resemblence so that it should be understood without explanation. An <em>earcon</em> is more symbolic, and has and arbitrary mapping of sound to meaning. Although Earcons are more flexible, they may have to be learned. The <em>spearcon</em>, a speech earcon, has the potential to offer the best of both worlds &ndash; flexibility and understandability. A <em>spindex</em> is a set of brief speech sounds that are used to navigate a long menu.</p>

<h3>Design Considerations</h3>

<p>Qualitative data may be better represented by dimensions of sound that are perceived as categorical, such as timbre. Whereas pitch or loudness, which are more continuous, may be better for ratio or interval data.</p>

<p>The polarity of the mapping matters. In one study, listeners agreed that pitch should increase with increasing temperature but that it should decrease with increasing size.</p>

<p>While the human hearing range stretches from about 20 Hz to 20,000 Hz, it may be more successful to scale data to the range where hearing is most sensitive, for example between 1000-5000 Hz.</p>

<p>A musical model, for example the notes on a piano, can provide a scale with perceptually equal steps. This convenience does come at the cost of resolution. A MIDI display using only notes 35-100 provides 65 points whereas a microtonal scale would be comparatively infinite.</p>

<p>Monitoring tasks require that the listener has a priori knowledge of a particular template so that they may recognise a sound and it&rsquo;s meaning.</p>

<p>Concurrent presentation of multiple data streams requires that the user be able to segregate the streams. Differences in timbre (musical instrument) or spatial separation (stereo panning) have been used for this purpose.</p>

<p>Context cues can aid perception. Like tick marks on the axis of a visual graph, a series of clicks can help the user keep track of time and a repeating reference tone can help with point estimation.</p>

<div class="well">
<p>The <a href="http://www.sonifyer.org/sound/erdbewegung/progress/?id=38">Sonification of Tohoku Earthquake</a> accelerates seismic activiy by a factor of 1440 to bring signal into the audible range.</a></p>
<div id="ytplayer"></div>
<script>
  // Load the IFrame Player API code asynchronously.
  var tag = document.createElement('script');
  tag.src = "https://www.youtube.com/player_api";
  var firstScriptTag = document.getElementsByTagName('script')[0];
  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

  // Replace the 'ytplayer' element with an <iframe> and
  // YouTube player after the API code downloads.
  var player;
  function onYouTubePlayerAPIReady() {
    player = new YT.Player('ytplayer', {
      height: '390',
      width: '640',
      videoId: '3PJxUPvz9Oo'
    });
  }
</script>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LEP Economic Profiles - How do local assets affect productivity]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2014/01/17/lep-economic-profiles-how-do-local-assets-affect-productivity/"/>
    <updated>2014-01-17T08:31:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2014/01/17/lep-economic-profiles-how-do-local-assets-affect-productivity</id>
    <content type="html"><![CDATA[<p>Jim Twomey of <a href="http://www.pion-economics.co.uk/">Pion Economics</a> put together a fascinating statistical analysis of Local Economic Partnership areas. I offered to visualise the findings, and here are the results (click through for the interactive visualisation):</p>

<p><a href="http://infonomics.ltd.uk/lep-profiles/index.html"><img src="http://infonomics.ltd.uk/news/images/post_images/lep_analysis_screenshot.png" title="LEP Analysis Screenshot" ></a></p>

<!-- more -->


<p>The statistical techqniue is quite advanced, and I&rsquo;m still searching for a simple way to explain it. The objective is to describe differences in productivity in terms of the attributes of each place so that performance may be understood, and insights found into what a place might need to improve. First the attributes are gathered together into asset groups that correlate closely with one another (factor analysis). Then the level of these 8 groups is compared against local economic productivity to estimate the contribution of those assets to performance. More information on the technique may be found in the <a href="http://pion-economics.co.uk/cms/resources/uploads/File/LEP_document.pdf">Developing LEP Economies Report</a>.</p>

<p>The interactive visualisation is designed to provide both an overview of the patterns across LEP areas and a means of finding the results that apply to each LEP individually. The analysis presents three panels. The map on the left is for navigation. By hovering over a given place, the relevant LEP profile will be selected. On the right, the top panel presents all LEPs ordered by productivity level and the bottom panel the contribution of assets. Each chart shows the overall range of values for context, with the selected LEP area highlighted in bold. You may also navigate using the top right panel.</p>

<p>The interactive element allows you to explore patterns and relationships. Try, for example, drawing your cursor from the periphery of England in towards London. Note how the level of productivity and the contribution of the Access asset group changes. Try comparing rural and urban areas.</p>

<p>We found that this visualisation technique makes it far more efficient to present research findings (displaying 39 profiles in one graphic) and allows additional insights to be discovered through interactive exploration.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to choose a Sample Size]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/10/18/how-to-choose-a-sample-size/"/>
    <updated>2013-10-18T09:26:00+01:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/10/18/how-to-choose-a-sample-size</id>
    <content type="html"><![CDATA[<p>I&rsquo;m often asked to provide advice on this question. The short answer is usually &ldquo;it depends&rdquo;. This is the longer answer that explains what it depends on!</p>

<p>The size of a sample doesn&rsquo;t need to be as large people usually think. The sample size doesn&rsquo;t actually depend upon the population size (i.e. the things &ndash; individuals, organisations, or projects &ndash; that the sample is supposed to represent). Indeed National opinion polls usually include no more than 1,000 responses (although they do require an <a href="http://en.wikipedia.org/wiki/Sampling_bias#Historical_examples">unbiased sample</a>).</p>

<p>So what does sample size depend upon..?</p>

<!-- more -->


<h2>Why choosing a sample size is important</h2>

<p>A sample is a subset of a population that you wish to study. A sample is typically chosen for research because it is too difficult or expensive to talk to the whole population. Fortunately statistical theory demonstrates that it is possible for responses taken from a sample to represent a wider population.</p>

<p>This post explains some of things to consider when choosing a representative sample.</p>

<h2>Total sample size</h2>

<p>The choice of overall sample size is driven by three statistical considerations</p>

<ul>
<li>what effect size you want to capture (i.e. the expected difference between the things you&rsquo;re comparing),</li>
<li>how willing you are to miss effects that are present, and</li>
<li>how willing you are to mistakenly identify effects that aren&rsquo;t present.</li>
</ul>


<p>We can set the latter two according to statistical convention (significance level of 95%, and a statistical power of 80%). Effect size really depends upon the context.</p>

<p>The main unknown here will be the standard deviation &ndash; that is to say the extent to which the things you&rsquo;re measuring vary from case-to-case. This, of course, determines the degree to which we can draw conclusions about the whole population by looking at a sample.</p>

<p>We define the effect size relative to the standard deviation. The larger the effect size, the smaller the sample needed to detect it. Drawing on social research, Cohen suggests that 20%, 50%, and 80% (of the standard deviation) represent small, medium, and large effect sizes respectively.</p>

<h2>Representativeness of sub-groups</h2>

<p>There are a couple of issues here. First, the overall results should be representative (you don&rsquo;t want to miss-out certain groups) and unbiased (if the sample structure doesn&rsquo;t match the proportion of the overall population the results will need to be weighted). Second, you might want to compare the results of the sub-groups (e.g. treatment vs control groups, cohorts by gender/ age/ ethncitiy, or to see if results change over time/ by location).</p>

<p>Bear in mind the above discussion on sample sizes. The rule of thumb value for sub-samples is to have 30 responses &ndash; is consistent with an effect size of 73%. This would, of course, be exhaustive for some of the smaller sub-groups. You may need to combine sub-groups or deal with this variability in the analysis rather than in the sample design &ndash; e.g. removing outliers after the responses have been gathered). You might aim for 30 responses among the larger groups but relax this for smaller ones.</p>

<h2>Response rates</h2>

<p>This depends on how and who contacts the projects. Fortunately, as a funder you&rsquo;re likely to have their attention! In my experience of contacting beneficiaries on behalf of public/ social projects, a 30% response rate is good going. Don&rsquo;t be surprised if you receive a response rate lower than 1% for an email survey (although the ease of contacting a wide range should mean this is still an effective method). I&rsquo;d aim to contact about 5 times as many people as you need responses.</p>

<p>The overall decision on sample size ought to take into account the cost and difficulty of acquiring responses and the observed variability of responses. You might want to bootstrap the process by running an initial small sample that would serve to identify whether further booster samples are required (e.g. &ldquo;the overall result is satisfactory but we&rsquo;re concerned about the apparent discrepancy among very large projects so they&rsquo;ll be the subject of further research&rdquo;). Bear in mind that this is by no means a precise science and this guidance should be considered in the context of practical concerns. Perhaps the most useful contribution of statistical advice &ndash; rather than identifying a magic number &ndash; is to help you to understand the overall patterns (variability matters, diminishing returns apply, very large/ small projects might need to be considered separately).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open Infrastructure - Tools for Smarter Citizens]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/10/17/open-infrastructure-tools-for-smarter-citizens/"/>
    <updated>2013-10-17T17:12:00+01:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/10/17/open-infrastructure-tools-for-smarter-citizens</id>
    <content type="html"><![CDATA[<p>This year the <a href="http://futureeverything.org/summit/summit-highlights/">Future Everything conference</a> focussed on three core themes: creative code, future cities, and the data society. In one of the final sessions (<a href="http://futureeverything.org/summit/conference/sessions/the-bespoke-smart-city/">The Bespoke Smart City</a>), keynote speaker Anthony Townsend set a challenge to the audience. He asked whether &lsquo;smart&rsquo; techniques were transferable from city to city, given that each city has it&rsquo;s own unique context. This article is a response to that question: I believe that there exists a set of processes and standards that may be replicated from place to place.</p>

<!-- more -->


<p><img src="http://infonomics.ltd.uk/news/images/post_images/routes_to_future.jpg"></p>

<p>One of the resounding messages from the conference was that smart cities are really a product of smart citizens. This is an idea close to my heart. I have written before about the need for <a href="{news/2009-02-13-people-managed-places">people-managed places</a>.</p>

<p>The top-down model of the smart city being advocated by technology corporations begins with a peculiar premise: that the city can be controlled. The engineering metaphor of a <a href="http://www.siemens.com/innovation/apps/pof_microsite/_pof-spring-2011/_html_en/city-cockpit.html">&lsquo;smart city cockpit&rsquo;</a> provides an illusion of centralised power but totalitarian control doesn&rsquo;t make people smart.</p>

<p>In reality cities are organic and messy. To make them smart we need to share information that citizens can use, not to capture data about people without their consent.</p>

<p>There exists a standard for sharing information openly so that everyone can use it: it&rsquo;s called the internet. We do not need proprietorial technology platforms or <a href="http://www.raptorsme.com/home/default.asp?page=welcome">urban operating systems</a>.</p>

<p>Here are some examples of good projects:</p>

<ul>
<li><em>Shared space</em> is a design principle which seeks to minimise the demarcations and barriers between pedestrian and vehicle traffice. The idea is that these barriers create risk compensation whereby road users act with less caution because the perceive a lower degree of risk. From a smart-citzens perspective, the urban planner has done their thinking for them. The shared space approach considers people to be intelligent and, if the possibility of danger is present, then people will be more cautious.</li>
<li><em>Xively</em> (previously Pachube/ Cosm) is a <a href="https://xively.co">platform for connecting and sharing data to/ from the internet of things</a>. Essentially this is an infrastructure for the physical city to talk to it&rsquo;s citizens.</li>
<li><em>Open 311</em> is a <a href="http://open311.org/">collaborative model and open standard for civic issue tracking</a>. It&rsquo;s a protocol for reporting problems to your local council. As a standard, economies of scale can be reached and the tools built around the platform may be shared from place to place.</li>
<li><em>Open Trip Planner</em> is a <a href="http://opentripplanner.com/">open source platform for multi-modal trip and itineray planning and analysis</a>. It is based upon Open Street Map and the (open) General Transit Feed Specification. It offers route planning, real time updates of delays/ disruptions, and analysis of coverage/ travel times etc.</li>
<li><em>The City Service Development Kit</em> is a <a href="http://www.citysdk.eu/">toolkit for the development of digital services in cities</a>. The tools build on top of Open Street Map and Open 311 to provide a Linked Open Data API.</li>
</ul>


<p>The common theme here is that these projects adopt open standards which allow us to learn from others and add our efforts to a common purpose. The tools themselves build around common standards and so may be customised to suit the specific context of each city (i.e. in terms of it&rsquo;s open street map layout).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open Data Sources - Beyond the Public Sector]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/03/20/open-data-sources-beyond-the-public-sector/"/>
    <updated>2013-03-20T09:40:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/03/20/open-data-sources-beyond-the-public-sector</id>
    <content type="html"><![CDATA[<p>This is the second in a pair of posts designed to provide a primer on sources of open data. This post focuses on non-public sector sources, the previous one looked at <a href="http://infonomics.ltd.uk/news/blog/2013/03/18/public-sector-information-whats-available-and-where-you-can-find-it/">where to go for open government data</a>.</p>

<p>This post was prepared as part of my contribution to the <a href="http://futureeverything.org/summit/conference/workshops-fringe-events/business-of-open-data-workshop/">Business of Open Data Workshop</a>. This workshop is the first in a series organised by <a href="http://opendatamanchester.org.uk/">Open Data Manchester</a> and <a href="http://futureeverything.org">Future Everything</a>.</p>

<!-- more -->


<h2>The Social Web</h2>

<p><img src="http://infonomics.ltd.uk/news/images/post_images/circles-bird.png"></p>

<p>As you might expect, there&rsquo;s a tremendous amount of data being generated across social networking platforms. These businesses thrive on having porous walls (from a data perspective at least). It&rsquo;s little surprise then that they all come with a suite of developer tools to help you build functionality into their offering. This approach wouldn&rsquo;t be considered by all to constitute openness (and indeed some are <a href="http://www.fsf.org/facebook">completely resistant to the prospect of Facebook owning their identity</a>) but it is worth mentioning as these resources contain a wealth of information. Typically permission must be given by a user for you to access their perspective on the social network using protocols such as open auth</p>

<ul>
<li><a href="https://dev.twitter.com/">Twitter&rsquo;s Developer APIs</a> provide programmatic access to what they call &lsquo;platform objects&rsquo;: Tweets, Users, Entities (#hashtags, media, urls, and @mentions), and Places.</li>
<li><a href="https://developers.facebook.com">Facebook for Developers</a> includes the Graph API for reading from and writing to Facebook platform: Users, Pages, Groups, Photos, Videos, and the actions between objects. They&rsquo;ve even got their <a href="https://developers.facebook.com/docs/technical-guides/fql/">own query language</a>!</li>
<li><a href="https://developer.linkedin.com/">Linked-in APIs</a> for People, Groups, Companies, and Jobs.</li>
<li><a href="https://developers.google.com/+/">The Google+ Platform</a> for People, Activities, Comments and Moments</li>
</ul>


<h2>Open Corporates</h2>

<p><img src="http://infonomics.ltd.uk/news/images/post_images/mapping-tesco.png"></p>

<p><a href="http://opencorporates.com/">OpenCorporates</a> has a straightforward (though big) ambition: to have a URL for every company in the world. The project combines data collected from official registries (such as Companies House in the UK) with crowd-sourced data (e.g. from unparseable filings etc). The <a href="http://blog.opencorporates.com/">blog</a> reveals some of the fascinating analyses that are possible (such as <a href="http://blog.opencorporates.com/2012/12/17/guest-post-data-sketching-with-the-opencorporates-api/">this network graph of directors and companies</a>) and issues that exist in this area (<a href="http://blog.opencorporates.com/2012/07/24/are-duns-numbers-the-crack-cocaine-of-id-systems-and-is-the-uk-the-latest-addict/">are DUNS numbers the crack cocaine of business identification?</a>).</p>

<h2>Princeton Wordnet</h2>

<p>The <a href="http://wordnet.princeton.edu/">Princeton WordNet</a> is a lexical database of English &ndash; a catalogue of words.</p>

<blockquote><p>Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated with the browser. WordNet is also freely and publicly available for download. WordNet&#8217;s structure makes it a useful tool for computational linguistics and natural language processing.</p></blockquote>


<p>If you&rsquo;re interested in text analysis then I highly recommend that you check out <a href="http://gate.ac.uk/">Gate</a> which of course has a Wordnet plugin.</p>

<h2>OPTA MCFC-Analytics</h2>

<p><img src="http://infonomics.ltd.uk/news/images/post_images/Opta.png"></p>

<p>Opta &ndash; a sports data company &ndash; has released data as part of the MCFC-Analytics project. There are two datasets:
&ndash; The &ldquo;Lite&rdquo; Dataset which has an entry for every player&rsquo;s appearance in each match (10,370 in total) from the 2011-12 Premier League Season. The 185 fields cover a handful of contextual details (name of player and team etc) and counts for all of the &ldquo;on ball&rdquo; events in some detail (e.g. &ldquo;Total Successful Passes Excl Crosses Corners&rdquo;). The data is provided in csv.
&ndash; The &ldquo;Advanced&rdquo; data provides a time and space (x, y, and z!) coded feed of events, codified by type. The initial set of open data covers only Manchester City players although more is promised if you can demonstrate some interesting developments&hellip;</p>

<h2>DBpedia</h2>

<p><img src="http://infonomics.ltd.uk/news/images/post_images/dbpedia_logo.png"></p>

<p><a href="http://dbpedia.org/">DBpedia</a>&hellip;</p>

<blockquote><p>&#8230;is a crowd-sourced community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to make sophisticated queries against Wikipedia, and to link other data sets on the Web to Wikipedia data. We hope this will make it easier for the amazing amount of information in Wikipedia to be used in new and interesting ways, and that it might inspire new mechanisms for navigating, linking, and improving the encyclopedia itself.</p></blockquote>


<h2>Freebase</h2>

<p><img src="http://infonomics.ltd.uk/news/images/post_images/freebase-logo.png"></p>

<p><a href="http://freebase.org/">Freebase</a> is a free, open knowledge graph covering 37 million topics, 2000 types, and 30,000 properties. Topics (e.g. <a href="http://www.freebase.com/m/04lg6">Leonardo da Vinci</a>) can have types (e.g. Visual Artists) and types contain properties (e.g. his <a href="http://www.freebase.com/query?autorun=1&amp;q=%5B%7B%22id%22:%22/m/04lg6%22,%22name%22:null,%22/visual_art/visual_artist/artworks%22:%5B%5D%7D%5D">artworks</a>).</p>

<p>Hopefully that gives you a flavour of what is out there. I&rsquo;d welcome any additional suggestions in the comments&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Public Sector Information: what's available and where you can find it]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/03/18/public-sector-information-whats-available-and-where-you-can-find-it/"/>
    <updated>2013-03-18T15:43:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/03/18/public-sector-information-whats-available-and-where-you-can-find-it</id>
    <content type="html"><![CDATA[<p>This is the first in a pair of posts designed to provide a primer on sources of open data. This post focuses on open government data and the next focusses on <a href="http://infonomics.ltd.uk/news/blog/2013/03/20/open-data-sources-beyond-the-public-sector">open data beyond the public sector</a>.</p>

<p>This post was prepared as part of my contribution to the <a href="http://futureeverything.org/summit/conference/workshops-fringe-events/business-of-open-data-workshop/">Business of Open Data Workshop</a>. This workshop is the first in a series organised by <a href="http://opendatamanchester.org.uk/">Open Data Manchester</a> and <a href="http://futureeverything.org">Future Everything</a>.</p>

<p>To understand why public sector information is the main source of open data you may wish to read about the <a href="http://infonomics.ltd.uk/news/blog/2012/10/25/the-economics-of-open-data">economics of open data</a>.</p>

<!-- more -->


<h2>Who is publishing Public Sector Information &ndash; Where can I find it?</h2>

<h3>data.gov.uk</h3>

<p><img class="left" src="http://infonomics.ltd.uk/news/images/post_images/dgu-header.png">
In 2009 <a href="http://data.gov.uk">data.gov.uk</a> was established to provide a central catalogue of public sector information, covering over 9,000 sources (as I write this). It provides meta data, that is, it acts an index of other data rather than a respository itself. The site is built on the <a href="http://okfn.org">Open Knowledge Foundation&rsquo;s</a> <a href="http://ckan.org/">CKAN</a> platform. It provides a <a href="http://data.gov.uk/sparql">SPARQL endpoint</a> for a <a href="http://data.gov.uk/linked-data/who-is-doing-what">variety of Linked Open Data resources</a>.
In theory, this site should cover most of what is available from Government. Although in the past, the coverage has been patchy in parts and duplicated in others, this has largely been resolved over the past few years. The main problem with data.gov.uk is that it&rsquo;s sometimes best to go direct to the horses&#8217; mouth particularly when what you&rsquo;re interested in is a delivery API, not metadata discovery. As such I&rsquo;ll go into a little more detail about the places you&rsquo;ll usually find yourself after you&rsquo;ve explored data.gov.uk.</p>

<h3>The Office of National Statistics</h3>

<p><img class="right" src="http://infonomics.ltd.uk/news/images/post_images/s300_NS_logo.jpg"></p>

<p>The Office for National Statistics is the executive office of the UK Statistics Authority. The authority is a dedicated non-ministerial government department with responsibility for assessing <em>Official statistics</em> (i.e. &ldquo;those produced by a government department or persons acting on behalf of the crown&rdquo;) against it&rsquo;s <a href="http://www.statisticsauthority.gov.uk/assessment/code-of-practice/">Code of Practice</a> to ensure that only compliant publications are designated <em>National Statistics</em>.</p>

<p>The <a href="http://www.ons.gov.uk">ONS</a> has long been providing data as both <a href="http://www.ons.gov.uk/ons/datasets-and-tables/index.html">datasets</a> and <a href="http://www.ons.gov.uk/ons/publications/index.html">published reports</a> (books, articles, bulletins) that include some narrative and interpretation. The most recent incarnation of the ONS site has convenient filters for discovering what available by theme, release date (including <a href="http://www.ons.gov.uk/ons/release-calendar/index.html">forthcoming releases</a>), and geographic scope and precision (i.e. where is covers and with what breakdowns). While it&rsquo;s naturally tempting to rush to the raw data you should also bear in mind that the <a href="http://www.ons.gov.uk/ons/guide-method/index.html">methodological guidance</a> is very important &ndash; there&rsquo;s many a time this has either saved me making a mistake in interpretation (fixed-capital investment figures used to be apportioned by jobs and so were no more informative for making regional comparisons) or suggested an alternate &lsquo;experimental&rsquo; dataset that offered new insights (e.g. multiple approaches to measuring Gross Value Added).</p>

<p>Of particular note are:</p>

<ul>
<li>the ONS&rsquo;s site for labour market statistics &ndash; <a href="http://www.nomisweb.co.uk">nomis</a> &ndash; they have a <a href="http://www.nomisweb.co.uk/api/v01/help">RESTful API</a> which is compliant with the Statistical Data and Metadata eXchange (SMDX) ISO standard. The API offers both discovery and delivery services, URI resolution, and HTML, XML, json, and csv response formats.</li>
<li>the <a href="http://www.neighbourhood.statistics.gov.uk/">Neighbourhood Statistics Exchange (NeSS)</a> &ndash; with their <a href="http://www.neighbourhood.statistics.gov.uk/dissemination/Info.do?page=nde.htm">Neighbourhood Data Exchange (NDE) API</a>, version 2 of which is now RESTful (I&rsquo;ve got some ruby bindings knocking around somewhere if you really want to use the SOAP interface instead).</li>
<li>the <a href="http://www.ons.gov.uk/ons/about-ons/what-we-do/programmes---projects/enhancing-access-to-ons-data/ons-api/index.html">forthcoming ONS API</a> which will operate under a similar principle and is designed with data from the 2011 Census in mind.</li>
</ul>


<h3>Central Government Departments</h3>

<p><img class="left" src="http://infonomics.ltd.uk/news/images/post_images/govuk-crest.png">
As part of their daily work and obligations for reporting, <a href="https://www.gov.uk/government/organisations">Central Government Departments</a> produce great volumes of data that is publically available. This departments have gradually be brought out from a heterogenous collection of sites operating under a department.gov.uk sub domain to a consistent www.gov.uk/department arrangement. This has made it much easier to search across Government for <a href="https://www.gov.uk/government/publications?publication_filter_option=statistics">statistics</a> or <a href="https://www.gov.uk/government/publications?publication_filter_option=research-and-analysis">research and analysis</a>. Indeed a convenient <a href="https://www.gov.uk/government/statistical-data-sets">index of statistical data sets</a> is provided.</p>

<h3>data.gov.*</h3>

<p><img class="right" src="http://infonomics.ltd.uk/news/images/post_images/datagm-beta.png">
There is also a plethora of other PSI datastores from <a href="http://www.datagm.org.uk/">Manchester</a> to <a href="http://data.gov.md/">Moldova</a> and, indeed, <a href="http://datacatalogs.org">all over the world</a>.</p>

<h2>What&rsquo;s available? The Big Names in PSI</h2>

<p><img src="http://infonomics.ltd.uk/news/images/post_images/oslogo.png"></p>

<ul>
<li>For transport, the main data sets are the <a href="">National Public Transport Access Nodes (NapTAN)</a>, which unique identifies public transport access points, and the <a href="http://data.gov.uk/dataset/nptdr">National Public Transport Data Repository</a>, which provides a snapshot of every GB journey in ATCO-CIF and TransXChange formats. Historically, the NPTDR has been taken annually in October but it is now being provided weekly as the <a href="http://traveline.info/tnds.html">Traveline National Dataset</a> (requires registration for FTP access).</li>
<li>The <a href="http://data.gov.uk/dataset/index-of-multiple-deprivation">Indicies of Multiple Deprivation</a> &ndash; which ranks English neighbourhoods (or Lower-layer Super Output Areas &ndash; LSOAs) according to seven dimensions of deprivation. The IMD is often used as a means of allocating resources by need. NB: the publication is irregularly timed and has (historically at least) been subject to methodological changes between years making it difficult to use this dataset for trend analysis. It does, however, provide an incredible level of geographic precision.</li>
<li>The <a href="http://data.gov.uk/dataset/coins">Combined Online Information System (COINS)</a> &ndash; which is basically the governments central accounts, all 4.3Gb of them!</li>
<li>The <a href="http://www.ons.gov.uk/ons/guide-method/geography/products/postcode-directories/-nspp-/index.html">ONS Postcode Directory and National Statistics Postcode Lookup</a> &ndash; which provides a lookup for transforming postcodes into other geographic terms including statistical areas (e.g. LSOAs) and geocodes (i.e. Latitude-Longitude/ Northings-Eastings).</li>
<li><a href="http://www.ordnancesurvey.co.uk/oswebsite/products/os-opendata.html">Ordnance Survey Open Data</a> including the &lsquo;tiles&rsquo; of the map (raster), government administrative boundaries (vector ESRI shapefiles), code-points (a csv of postcode coordinates etc), and gazetteers of place and road names.</li>
<li><a href="http://legislation.gov.uk">Legislation.gov.uk</a> publishes all UK legislation in both the original enacted form and with revisions that are made over time. The platform is managed by the <a href="http://www.nationalarchives.gov.uk/">The National Archives</a> and provides Linked Data access (try adding /data.xml to the end of a URI).</li>
</ul>


<p>If you&rsquo;re interested in more of the top datasets you might like to peruse <a href="http://data.gov.uk/data/site-usage/dataset">data.gov.uk&rsquo;s own list of popular datasets</a> or read on to <a href="http://infonomics.ltd.uk/news/blog/2013/03/20/open-data-sources-beyond-the-public-sector">find out about what&rsquo;s available outside of the public sector</a>.</p>

<p>Have I missed some major sources or data sets? Let me know in the comments&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open Data Business Models]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/03/01/open-data-business-models/"/>
    <updated>2013-03-01T19:16:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/03/01/open-data-business-models</id>
    <content type="html"><![CDATA[<p>Data doesn&rsquo;t make for a good tradable commodity, as discussed in a previous post on the <a href="http://infonomics.ltd.uk/news/blog/2012/10/25/the-economics-of-open-data">Economics of Open Data</a>. Because of these market failures, there&rsquo;s a strong business case for the public sector to open data.
But what about commercial business models for open data? This post explains why it&rsquo;s not just the public sector who ought to pay attention to this opportunity.
The post is split into three topics: opening data that you own (why data assets are more valuable to you open), using open data (how to exploit your or someone else&rsquo;s open data) and commercial models (how to get paid in the process).</p>

<!--more-->


<h2>Opening your data</h2>

<p>Why should you open your data? Here are a few reasons:</p>

<ul>
<li>Network effects</li>
<li>Risk mitigation</li>
<li>Reputation</li>
<li>Attention</li>
</ul>


<h3>Network effects &ndash; Open is more exploitable</h3>

<p>There is a much greater potential for exploting data where more people have access. As a result your data will have greater value when it is open (to see how see below). Although you may have to share some of the pie with others, the overall pie (and so potentially your slice too) will be bigger.
This is particularly obvious for businesses who have ambitions for their data that are beyond their reach. It&rsquo;s clear that small businesses may not be able to afford a dedicated resource for data manipulation or scientific analysis. It&rsquo;s also true of larger organisations where the analysis is particularly complex and requires the skills of a specialist.
The notion of a network-effect actually goes a step further to argue that it&rsquo;s impossible for any individual business to realise the full potential of their data internally. The data has value for each person that uses it, and in turn each use increases the value of the data. This is because data doesn&rsquo;t hold a value <em>per se</em> &ndash; it&rsquo;s value is only realised when data is interpreted with context as information (i.e. when a datum is combined with another datum). If the openness is reciprocal (share-alike) then your data will increase in value as it is shared and linked with other sources. You may even reasonably expect to have your data corrected, cleaned, and maintained (reducing the cost of ownership). There is also the possiblity of other businesses being able to extract value with niche customisations that you don&rsquo;t have the interest (let alone the capacity or knowledge) to consider.
It should be apparent that this benefit is not guaranteed in all situations. If the pie doesn&rsquo;t get any bigger then there may be little to gain from sharing it. The likelihood is that it may be very hard to predict all possible avenues for exploiting your data. Further it&rsquo;s not certain, therefore that keeping data closed a precaution is prudent &ndash; it may turn out to be short-sighted&hellip;</p>

<h3>Risk mitigation &ndash; Open it or else</h3>

<p>If you don&rsquo;t open it, your competitors will.
The value of information depends upon it revealing something novel. Since there are usually a variety of ways of answering a given question it&rsquo;s likely that your data is not entirely unique (i.e. for every purpose, and every user). As a result you stand to be undermined by a competitor who is willing to offer alternative (although not necessarily completely substitutable) data at a lower price. Economic theory suggests that, in competitive markets, the price will equal the marginal cost of supply. Since the marginal cost of supplying data is effectively 0 (or at least very low), you may well find yourself undercut by an open data provider. You are only insulated from this risk to the extent that alternative sources are not perfect substitutes for your data from the perspective of your customers and markets.
Again, this risk is not certain. If you have the sole provider of a data source for which there are no direct substitutes then the probability of being undercut is lower. By exploiting this monopolistic position, however, you leave tremendous incentives for others to find imaginative alternatives to your offering. It may turn out that you&rsquo;re not as unique as you&rsquo;d assumed. Indeed there&rsquo;s no guarantee that you&rsquo;ll sustain the position of sole provider. Changing technology means that markets that were once protected by barriers to entry are now open to competition. If you are to operate an open data business model, however, and focus on a complementary service, then you are much more likely to cement your position as the default provider and retain your standing in the market&hellip;</p>

<h3>Reputation &ndash; Open is credible</h3>

<p>Opening data is a powerful means of being transparent. That transparency will make your propositions credibile by sending the signal that you have nothing to hide.
As has been particularly obvious in the food industry of late (for historians reading this blog in the next millenium we&rsquo;re in the middle of a crisis about eating horse meat that was advertised as beef), the complexity of global supply chains bring risks because provenance is hard to establish. Open data presents a solution to this problem.
This has to be genuine. Releasing data under the pretense of openness and at the same time making it difficult for anyone to interpret the data will be recognised as a cynical gesture.</p>

<h3>Attention &ndash; Open is attractive</h3>

<p>Where data is opened in an easily accessible format with some guarantee that the openness will be sustained, it will gather a lot of attention. A captive audience is certainly a valuable asset. The traffic it generates may be more valuable than the data itself. The technical implementation must be thought through carefully if the attention is going to be captured/ channeled through this approach.</p>

<h2>Using Open Data</h2>

<p>There are a variety of ways to profit from open data. The line-between this categories is blurred. The general sequence is from data source to final use. Applications may include one or more steps.</p>

<ul>
<li>Enabling Infrastructural</li>
<li>Information Enrichment</li>
<li>Analysis and Consultancy</li>
<li>Application Development</li>
</ul>


<h3>Enabling Infrastructure</h3>

<p>Although open data ought to be free to access it may not be free to enable that access. There is a cost (and therefore a profit) associated with opening data. A business or a government agency who wishes to open their a data are customers in this regard. Thus there is a business opportunity in supplying services such as:</p>

<ul>
<li>Information architecture design &ndash; planning out the schema, ontology, or technical implementation details for open data (either as a service or as creation of an open meta-data asset);</li>
<li>Database &ndash; the technology that will hold the data;</li>
<li>Web hosting &ndash; the technology that will open-up access to the data;</li>
<li>Service/ Application layer &ndash; the technology that will make it (re)usable: Linked-open data, indexing services, user interfaces, APIs, integration gateways, data loaders etc (note that I&rsquo;m thinking particularly of just the server-side &lsquo;openning&rsquo; bit here, not client-side applications of open data &ndash; that comes next!);</li>
<li>Advice on Licensing &ndash; the legal framework; and</li>
<li>Business Advice on Strategy &ndash; as per this post!</li>
</ul>


<h3>Information Enrichment</h3>

<p>As we have seen, data is valuable when presented in context as information. There is a value then, in enriching data by adding more data or context. Some ideas for what this might look like are shared below. The general distinction about enrichment I&rsquo;ve made is that it stops short of being analysis (i.e. the output of enrichment is more open data, not conclusions or decisions &ndash; see the following section for that).
Enrichment services might include:</p>

<ul>
<li>Linking-open data sources (tying a dataset into the semantic web)</li>
<li>Quality assurance &ndash; providing an independent certification or compliance service (must be careful to consider who pays/ scrutinises for the sake of credibility)</li>
<li>Text analysis &ndash; information extraction (entity recognition, annotation)</li>
<li>Cleaning data &ndash; e.g. reconciling duplicates, ensuring consistency, identifying junk data, processing data so that it conforms with an objective standard)</li>
<li>Reformating &ndash; i.e. tranforming data so that it&rsquo;s provided in a more useful (typically machine readable) format e.g. translating pdfs into csv, wrapping a database in an API</li>
</ul>


<h3>Analysis and Consultancy</h3>

<p>Open data can be a vital input into services designed to provide advice and support to decision makers. Beyond the simple fact that the information is free, there is a value in using a resource that is open because it is verifiable. That is to say, your analysis may be reproduced and checked. Notice that this doesn&rsquo;t actually need to have happened for this to be of benefit, indeed the very possibility may be credible enough.
For the sake of completeness here are some examples of analytical services that might benefit from using open data sources:</p>

<ul>
<li>Consultancy/ Advice/ Decision support/ Insight</li>
<li>Policy Development/ Advocacy/ Lobbying</li>
<li>Statistical analysis and modelling</li>
<li>Presentation (data graphics/ visualisation, sonification, and interaction)</li>
<li>Scientific, Technical or Operational research (and product or process development)</li>
</ul>


<h3>Application Development</h3>

<p>Finally open data can be integrated into applications that provide value to consumers, businesses, government, or indeed other developers. Here open data is interpreted in a particular domain of interest to provide a service.</p>

<h2>Commercial models for Open Data</h2>

<p>So we&rsquo;ve seen the value of openning data, and of using open data, but how can this value be translated into profit? Here I&rsquo;ll present a few models:</p>

<ul>
<li>normal pricing</li>
<li>cross-subsidy</li>
<li>freemium</li>
<li>advertising</li>
<li>sponsorship</li>
<li>affiliation</li>
</ul>


<h3>Normal pricing</h3>

<p>The most obvious model is &lsquo;normal&rsquo; pricing. This bears stating as it&rsquo;s not always apparent from listening to open data advocates! Just because the data is provided for free, it doesn&rsquo;t mean that the services that apply this data must be provided for free. Your ability to charge for something will depend upon how costly it would be for another person (or business) to reproduce the application you&rsquo;ve created.
Traditional pricing models are also part of&hellip;</p>

<h3>Cross-subsidy</h3>

<p>The idea here is that an open data proposition enhances the value of complementary services to the extent that those other services make enough extra profit to pay for the costs of openning the data. Clearly this is easiest where the organisation that decides to open the data is also the one generating profit from the service although it may be possible to design commercial arrangements for a payment between two organisations.
The inherent difficulty with this model is problem of quantifying the contribution of the open data to the complementary service. Since open data is untraded and thus there&rsquo;s no price signal of value. While it may be possible to calculate the total value of network effects captured by a given organisation, it is particularly difficult to put a price on the risk avoidance or reputational benefits.</p>

<h3>Freemium</h3>

<p>Where the service is the provision of data itself it&rsquo;s not possible to follow either of the first two models. As such an alternative is to segment the market &ndash; providing a basic level of &lsquo;free&rsquo; service and a premium level of &lsquo;paid&rsquo; service (essentially a cross-subsidy from the latter to the former). This can operate like a loss-leader. Typically the market is segmented according to the level of access provided (number or range of requests permitted or the speed of response). As the premium services begin to differ systematically from the core open data offering (e.g. training or consultancy) this pricing model begins to ressemble the cross-subsidy archetype.</p>

<h3>Advertising</h3>

<p>This is the most generally-applicable of the pricing models. Where a cross-subsidy cannot be established (perhaps because the difficulty agreeing a contract between the data opener and service vendor) it may be possible to pay for open data via advertising.</p>

<h3>Sponsorship</h3>

<p>If your data generates enough attention, people may be willing to pay you to include their content. Naturally this isn&rsquo;t an obvious option for a start-up or un-tested service and it may be inappropriate for some data sets. For example: sponsored links on Twitter, Google Search and Reddit.</p>

<h3>Affiliation</h3>

<p>This is the reverse side of sponsorship where affiliates receive open data for free which they then process and redistribute in order to send attention back up the chain. For example: Amazon.</p>

<h2>Further information&hellip;</h2>

<p>If you want to find out more then you might like to check out these links:</p>

<ul>
<li><a href="http://www.slideshare.net/OpeningUp/open-data-conference-john-sheridan-open-data-as-an-operating-model">Open Data as an Operating Model</a> &ndash; John Sheridan explains how a linked-open-data approach enabled a mutually beneficial ecosystem around legislation.gov.uk. Check out slides 17, 24, and 33 in particular &ndash; they should you how the pieces fit together.</li>
<li><a href="http://chiefmartec.com/2010/01/7-business-models-for-linked-data/">7 Business Models for Linked Data</a> &ndash; Scott Brinker provides some far more catchy labels for these ideas than I have! <em>Data-layer ads</em>?!</li>
<li><a href="http://blog.ldodds.com/2010/01/10/thoughts-on-linked-data-business-models/">Thoughts on Linked Data Business Models</a> &ndash; Leigh Dodds provides some insights into how Scott&rsquo;s ideas might be implemented and also comments on a motive I&rsquo;ve overlooked here &ndash; philanthropy!</li>
<li><a href="http://www.scribd.com/doc/124951288/The-business-of-Open-Data-where-s-the-benefit">The Business of Open Data &ndash; Where&rsquo;s the Benefit</a> and <a href="https://soundcloud.com/theodi/odi-fridays-the-business-of">the accompanying audio track</a> &ndash; Jeni Tenison provides a contrast of a few models (including the closed data case) with the help of Osterwalder&rsquo;s Business Model Canvas.</li>
</ul>


<p>If that&rsquo;s not enough then you might be interested in attending a workshop on the <a href="http://futureeverything.org/summit/conference/business-of-open-data-workshop/">Business of Open Data</a> that&rsquo;s taking place as part of this year&rsquo;s <a href="http://futureeverything.org/">Future Everything Summit of Ideas &amp; Digtial Invention</a>.</p>

<p>Indeed if you&rsquo;re in Manchester why not pop along to the next <a href="http://opendatamanchester.org.uk/">Open Data Manchester</a>?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[9 Tools Every Information Analyst Should Have]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/01/30/9-tools-every-information-analyst-should-have/"/>
    <updated>2013-01-30T12:15:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/01/30/9-tools-every-information-analyst-should-have</id>
    <content type="html"><![CDATA[<p>I was interested to read Rebecca Murphey&rsquo;s <a href="http://rmurphey.com/blog/2012/04/12/a-baseline-for-front-end-developers/">Baseline for Front End Developers</a>  in which the author realizes that she has begun to take for granted a basic set of tools of the trade. She presents &ldquo;a few things that [she] wants to start expecting people to be familiar with, along with some resources you can use if you feel like you need to get up to speed&rdquo;. I&rsquo;d like to pay homage to that article and revise it in light of <em>the tools I believe are essential for an Information Analyst</em>. There is considerable overlap in the tools suggested although the application will differ.</p>

<!--more -->


<p><img src="http://imgs.xkcd.com/comics/regular_expressions.png" title="Wait, forgot to escape a space.  Wheeeeee[taptaptap]eeeeee." alt="XKCD Regular Expressions Comic"></p>

<h2>GNU/ UNIX Command Line Tools</h2>

<p>These tools are especially powerful because they don&rsquo;t require you to load the entire file into memory. This makes it possible to work with very large files efficiently.</p>

<ul>
<li><code>head</code>, <code>tail</code>, <code>less</code>, <code>cat</code> etc for looking at files</li>
<li><code>awk</code>, <code>sed</code>, and <code>tr</code> for processing and altering files</li>
<li><code>cut</code>, <code>paste</code>, and <code>join</code> again for editing and combining text files</li>
<li><code>sort</code> and <code>uniq</code></li>
<li><code>wc</code> for counting lines/ words/ characters</li>
<li><code>iconv</code> for converting between different encodings</li>
</ul>


<p>Some useful resources include:</p>

<ul>
<li><a href="http://blog.sanctum.geek.nz/series/unix-as-ide/">Unix as an IDE</a></li>
<li><a href="http://www.ibm.com/developerworks/linux/library/l-textutils/index.html">Simplify data extraction using Linux text utilities</a></li>
<li><a href="http://tldp.org/LDP/abs/html/textproc.html">Text Processing Commands in the Advanced Bash-Scripting Guide</a></li>
<li><a href="http://funarg.nfshost.com/r2/notes/sed-return-comma.html">Replacing returns with commas in Sed</a></li>
<li><a href="https://github.com/robbyrussell/oh-my-zsh">Oh my zsh</a> &ndash; zshell a replacement for bash</li>
</ul>


<h2>Regular Expressions</h2>

<p>Regexs are extremely powerful, if sometimes counter-intuitive. They are vital for defining patterns and replacements in text processing. Although some <a href="http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454">debate</a> exists a to whether they should be used to parse structured formats like XHTML, they are certainly a vital tool.</p>

<p>Having said which, I find this quote pretty amusing!</p>

<blockquote><p>Some people, when confronted with a problem, think “I know, I&#8217;ll use regular expressions.” Now they have two problems.</p><footer><strong>Jamie Zawinski</strong> <cite><a href='http://regex.info/blog/2006-09-15/247'>Jeffrey Friedl&#8217;s Blog</a></cite></footer></blockquote>


<ul>
<li><a href="http://gskinner.com/RegExr/">RegExr</a> &ndash; Regex building and testing tool</li>
<li><a href="http://www.regexper.com/">Regexper</a> &ndash; visualisation tool</li>
<li><a href="http://www.regular-expressions.info/">Reference Docs</a></li>
<li>An example for <a href="http://www.softwareprojects.com/resources/programming/t-validate-a-credit-card-1682.html">validating credit card numbers</a> or <a href="http://stackoverflow.com/a/164994/187009">validating UK Postcodes</a></li>
</ul>


<h2>A Programming/ Scripting Language</h2>

<p>If nothing else, these provide the glue to connect different sections of your data pipeline. The incredible range of libraries avavailable to most scripting languages will make quick work of many common information processing tasks: parsing, spidering, <abbr title="Extraction, Translation, and Loading">ETL</abbr> etc. The choice of language is down to personal preference or compatibility with the team.
It should suffice to say that knowledge of a scripting language is necessary and I don&rsquo;t feel the need to recommend a particular language. Having said which, I can&rsquo;t resist saying that I prefer <code>ruby</code> but have dabbled in <code>perl</code> and <code>python</code>. I find the following ruby gems indispensible: <code>mechanize</code>, <code>nokogiri</code>, <code>fastercsv</code> (now <code>CSV</code> in the 1.9 standard library), and <code>json</code>. I&rsquo;ve also been using <code>javascript</code> and node for scripting outside of the browser as asynchronous processing has proved to be very quick (if a little peculiar to code at first).</p>

<ul>
<li><a href="http://mislav.uniqpath.com/poignant-guide/book/chapter-1.html">Why The Lucky Stiff&rsquo;s (poignant) Guide to Ruby</a></li>
<li><a href="http://www.rubyist.net/~slagell/ruby/">A guide written by Matz (the original author of Ruby)</a></li>
<li><a href="https://github.com/styleguide/ruby">Github&rsquo;s Ruby Style Guide</a></li>
<li><a href="https://www.ruby-toolbox.com/">Ruby Toolbox</a> &ndash; for finding and selecting gems</li>
<li>John Ressig&rsquo;s <a href="http://ejohn.org/apps/learn/">Learning Advanced Javascript</a></li>
</ul>


<h2>Text data files</h2>

<p>Probably the most basic technology used in information analysis, the flat file is the lowest common denominator. You can almost guarantee that everyone will be able to cope with <code>csv</code> files (even if they need to be translated before loading into another tool). The expressiveness and rigour of <code>XML</code> makes it another vital tool although it&rsquo;s rapidly being superceded in bandwidth-conscious realm of http by the less verbose <code>json</code>.
Information analysts ought to be able to handle (read, understand, parse and translate) files in these all of formats even if they immediately convert them to a prefered type.</p>

<ul>
<li><a href="http://www.delicious.com/redirect?url=http%3A//andrewjwelch.com/code/xslt/csv/csv-to-xml_v2.html">XSLT for CSV 2 XML</a></li>
<li><a href="http://www.w3schools.com/xpath/xpath_syntax.asp">XPath Syntax</a></li>
<li><a href="http://www.evagoras.com/2011/02/10/improving-an-xml-feed-display-through-css-and-xslt/">Improving an XML feed through CSS and XSLT</a></li>
<li><a href="http://www.json.org/">JSON reference with links to parsers</a></li>
</ul>


<h2>Relational Databases, SQL, and NoSQL</h2>

<p>This shouldn&rsquo;t really come as a surprise to anyone. If you&rsquo;re going to analyse information, you&rsquo;re going to need to deal with databases. An analyst ought to be able to write <abbr title="Structured Query Language">SQL</abbr>, understand normalisation/ denormalisation, and possibly also have some knowledge of <abbr title="Object-Relational Mapping">ORM</abbr>. NoSQL databases are also useful when the data structure is better represented by document, graph or key:value structures (and where tables would be very sparse).</p>

<ul>
<li><a href="http://dev.mysql.com/">MySQL</a></li>
<li><a href="http://www.nparikh.org/unix/mysql.php">MySQL Cheat Sheet</a></li>
<li><a href="http://www.sqlite.org/">SQLite</a></li>
<li><a href="http://en.wikipedia.org/wiki/Database_normalization">database normalisation</a> and <a href="http://www.codinghorror.com/blog/2008/07/maybe-normalizing-isnt-normal.html">denormalisation</a></li>
<li><a href="http://nosql-database.org/">NoSQL Guide</a></li>
<li><a href="http://www.mongodb.org/">Mongodb</a> &ndash; document database</li>
<li><a href="http://www.neo4j.org/">Neo4j</a> &ndash; graph database</li>
<li><a href="http://couchdb.apache.org/">CouchDB</a> &ndash; json + map reduce + http</li>
</ul>


<h2>Statistical Library</h2>

<p>Unless you really want to code algorithms for every statistical process from scratch, you&rsquo;re going to want to adopt and familiarise yourself with a statistical library. I would strongly recommend <code>GNU-R</code> although other options include <code>Stata</code>, <code>SPSS</code>, and <code>Matlab</code> etc. Of course there is probably a library available for your prefered programming language. <code>scipy</code> and <code>numpy</code> are very popular choices for <code>python</code>.</p>

<ul>
<li><a href="http://www.r-project.org/">The R Project</a></li>
<li><a href="http://rseek.org">R Seek</a> &ndash; R-specific search engine</li>
<li><a href="http://stackoverflow.com/tags/r">R tag on Stackoverflow</a> (bonus: <a href="http://stats.stackexchange.com/">Cross Validated</a> &ndash; the stats-specific Stack Exchange Q&amp;A site)</li>
<li><a href="http://blog.revolutionanalytics.com/">Revolution Analytics Blog</a></li>
<li><a href="http://docs.ggplot2.org/current/">ggplot2</a></li>
<li><a href="gretl.sourceforge.net">gretl</a> &ndash; Gnu Regression, Econometrics and Time-series Library</li>
</ul>


<h2>Visualisation Framework</h2>

<p>Similarly you&rsquo;ll probably want to use some sort of framework for generating visualisations. I&rsquo;ve tried many and have found myself coming back to a few:</p>

<ul>
<li><a href="http://d3js.org">d3 data driven documents</a> &ndash; A JavaScript library for manipulating documents based on data using HTML, SVG and CSS</li>
<li><a href="http://docs.ggplot2.org/current/">ggplot2</a> &ndash; the tool that usually attracts people to R; excellent for quickly sketching out data graphics</li>
<li><a href="http://processing.org">processing</a> &ndash; Processing is an electronic sketchbook for developing ideas &ndash; in Java and now Javascript too.</li>
</ul>


<h2>Report Templating</h2>

<p>Usually you&rsquo;re going to want to place your analysis in context with some commentary. If you&rsquo;re going to be repeating a given report &ndash; either in part or in full &ndash; they you stand to benefit greatly from using some form of template. Again this will depend upon your prefered programming languages. My favoured tools are listed below.</p>

<ul>
<li><a href="http://www.stat.uni-muenchen.de/~leisch/Sweave/">Sweave</a> &ndash; R + Latex and <a href="http://rss.acs.unt.edu/Rdoc/library/odfWeave/html/odfWeave.html">Open Document Format</a>. I&rsquo;ve not tried it yet but <a href="http://yihui.name/knitr/">Knitr</a> looks like it could be a useful package).</li>
<li><a href="http://haml.info/">HAML &ndash; HTML abstraction markup language</a> &ndash; very clear markup with indentation (so you don&rsquo;t have to write out every tag twice), you&rsquo;re able to use ruby inline.</li>
<li><a href="http://sass-lang.com">Sass &ndash; Syntactically Awesome Stylesheets</a> &ndash; an extension of CSS3, adding nested rules, variables, mixins, selector inheritance, and more.</li>
<li><a href="http://garann.github.com/template-chooser/">Javascript Template Engine Chooser</a></li>
<li><a href="http://d3js.org">d3 data driven documents</a> &ndash; for programmatically creating html from data</li>
</ul>


<h2>Version Management</h2>

<p>Version management is vital for tracking progress, making experimental branches in your work, and coordinating across teams. The clear leader in this regard is <a href="http://git-scm.com">git</a> and the amazing project hosting environment <a href="https://github.com/">github</a>. I&rsquo;ve got in the habit of versioning everything &ndash; not just my code &ndash; but my home file configurations too.</p>

<h2>And the rest&hellip;</h2>

<p>This is just a quick list I wrote off the top of my head. What have I missed? What else do you use?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Information Failure is not a Business Case]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2013/01/16/information-failure-is-not-a-business-case/"/>
    <updated>2013-01-16T14:39:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2013/01/16/information-failure-is-not-a-business-case</id>
    <content type="html"><![CDATA[<p>Ignorance is no excuse.</p>

<p>Here I&rsquo;m referring to a public sector business case &ndash; i.e. a rationale for intervention in the economy based upon market failure. Not a business model. Information failure may indeed be <a href="http://infonomics.ltd.uk/news/blog/2012/11/13/open-for-business/">a valid business model for adding value in the private sector</a>.</p>

<!--more-->


<p>It&rsquo;s all too common to see ignorance being used as a rationale for a public project &ndash; &ldquo;if only more people knew about X then society/ the economy would be improved&rdquo;. If there are gains to trade available through better knowlege, then there are (rational) incentives for private individuals to learn. If X is a public sector project then this is actually marketing &ndash; justifiable on strategic grounds but not on the basis on market failure. If X is an intrinsically or publicly desirable course of action (e.g. not smoking) then the relevant market failure is actually the <strong>merit good</strong> case which is based on people having accurate information but choosing to ignore it &ndash; here again, information provision is actually a type of marketing.</p>

<p>The business case for information-based interventions should actually be based upon a different type of market failure: <strong>assymetric information</strong>. This occurs when either the buyer or seller knows more about the trade than the other. This can take the form of hidden information (you don&rsquo;t know the used-car salesman is selling you a lemon) or hidden action (your insurance company doesn&rsquo;t know whether you lock your door/ set the alarm every time you go out).  This imbalances leads to <strong>adverse selection</strong> (in the case of hidden information) or <strong>moral hazard</strong> (in the case of hidden action). Either way, the good quality products or services cannot be distinguished from the bad quality ones on the basis of price (the mechanism through which all markets operate).</p>

<p>Assymetric information is not simply solved by information provision. This market failure creates a business case for redressing the imbalance in information. It might include the creation of accreditation schemes or quality standards that, if credible, will buyers or sellers signal their quality or screen the quality of their trading partners.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pay for outcomes not activity]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/19/pay-for-outcomes-not-activity/"/>
    <updated>2012-12-19T20:37:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/19/pay-for-outcomes-not-activity</id>
    <content type="html"><![CDATA[<p>Commission outcomes not activity.</p>

<p>As part of my contribution to <a href="http://thecreativeexchange.org/">The Creative Exchange</a> I attended a workshop on Public Service Innovation and Democracy. Through the course of discussion we soon discovered that several attendees working in creative and service sectors experience a common problem with tendering &ndash; that the organisation running the procurement often has very fixed ideas about what they want to buy. There was general agreement that this tendency to be very specific didn&rsquo;t leave much room for imagniation and innovation.</p>

<p>The outcome of our discussion was a proposal to explore a new approach to commissioning digital and creative services.</p>

<!--more-->


<p>Here&rsquo;s a quote from the terms of reference:</p>

<blockquote><p>The standard tendering model is not effective in the contemporary cultural and creative sector:</p>

<ul>
<li><strong>Premature-specification</strong>: The traditional model, with a specification of how or what should be delivered, is appropriate to homogenous services or goods where tenders must meet requirements that can be determined before the competitive procedure. Creative services, by contrast, are constrained if the method is defined in advance. Instead the commissioner should focus on explaining their aims and objectives and allow the competing suppliers to imagine and suggest what and how these should be delivered.</li>
<li><strong>Speculative work</strong>: procurement of creative services can involve requests for ideas, examples, or other forms of speculative work like open competitions. This involves considerable upfront investment from suppliers, the intellectual property rights for which cannot be protected. Indeed the award of a contract is not guaranteed overall (let alone to any given supplier). Suppliers are at a significant disadvantage in this commissioning model and the incentives for participation are weak.</li>
<li><strong>Risk-aversion</strong>: The priority in procurement is often to minimise risk rather than maximise value-for-money. This can lead to a bias towards larger established firms at the expense of SMEs and of innovation.</li>
<li><strong>Administrative-burden</strong>: public tenders often involve lengthy pre-qualification processes and masses of paperwork (often owing to risk aversion as above). This burden weighs disproportionately heavily on SMEs who may not have the resources to respond or for whom such requirements may be irrelevant (i.e. a freelance designer working from her kitchen probably doesn’t need ISO compliant Health and Safety policy).</li>
</ul>


<p><cite>Alpha Procurement Proposal</cite></p></blockquote>

<p>The project is at a very early stage but I&rsquo;ll provide an update once we&rsquo;re underway.</p>

<p>If you&rsquo;ve any thoughts by all means let us know in the comments&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[3 Metrics for Heath Economics Analysis]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/19/3-metrics-for-heath-economics-analysis/"/>
    <updated>2012-12-19T16:43:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/19/3-metrics-for-heath-economics-analysis</id>
    <content type="html"><![CDATA[<p>I&rsquo;m often asked to suggest measures for assessing health interventions from an economic perspective. Here are explanations of four commonly used analytical methods:</p>

<ul>
<li>Reduction in Costs of Treatment</li>
<li>Cost per Quality Adjusted Life Years (QALY)</li>
<li>Incremental Cost Effectiveness Ratio (ICER)</li>
</ul>


<!--more-->


<p>As should be obvious from the titles, cost plays a significance role in health economics analysis. The first measure considers cost directly. The second measures also incorporate utility or benefit. The final measure should be used for interpretation.</p>

<p>In practice, it&rsquo;s advisable to consider using more than one method to account for the limitations of individual measures. It&rsquo;s also important to control for demographic and socio-economic characteristics such as age, gender, ethnicity, employment, and health condition. It will also be insightful to consider location as a proxy indicator for a range of (highly-correlated) socio-economic characteristics.</p>

<h2>Reduction in costs of treatment</h2>

<p>This is the most easily understandable economic impact of health interventions. If preventive steps are taken, fewer people contract disease, and less will need to be spent on treatment. The main focus of research under this measure is in <strong>quantifying the intervention&rsquo;s impact on health and the cost of treatment that is obviated</strong>. Weight loss, for example, will reduce the relative risk of diabetes and cardio-vascual disease. The relationship between intervention and disease risk might be established by reference to physiological measures (such as Body Mass Index). Cost of treatment might include the cost of prescription medicine, GP consultation, surgery and palliative care.</p>

<p>The difficulty with this sort of measure is that the impact is stated in terms of money that would otherwise have had to be spent and, as such, is quite intangigble.</p>

<h2>Cost per QALY</h2>

<p>A quality adjusted-life year, or QALY, represents <strong>the number of years of life that would be added by an intervention, taking into account the quality of those years</strong>. If the extra years would not be lived in full health they are given a value less than 1. Clearly the method of adjustment for quality is critical to this metric.
A commonly adopted measure is the <a href="http://www.euroqol.org/">EQ-5D</a> scale which seeks to categorise health states according to 5 dimensions: mobility, self-care, usual activities, pain/ discomfort and anxiety/ depression. This is combined with a quantiative measure of the patients self-assessment of their health on a <em>vertical analogue scale</em>.
Other approaches include the <em>time trade-off</em>, where respondents are asked to choose between remaining in an ill state or being restored to perfect health with a shorter life expectancy, and <em>standard gamble</em> where the alternative could restore them to perfect health or kill them.
This measure is criticised due to the difficulty of establishing a meaningful, objective, and comparable definition of &ldquo;perfect health&rdquo; and &ldquo;disease burden&rdquo;.</p>

<h2>Incremental Cost-Effectiveness Ratio</h2>

<p>This ratio is used for comparing intervention options on a like for like basis. The Incremental Cost Effectiveness Ratio (ICER) is a <strong>comparison of the relative cost per QALY of the intervention and a reference case</strong> as follows (where QALY refers to a Quality Adjusted Life Year):
$$
ICER = \frac{ \text{£ Cost of intervention} &ndash; \text{£ Cost of reference case} } { \text{QALY Effect of intervention} –  \text{QALY Effect of reference case} }
$$
The ICER assumes a multiplicative model for QALYs (i.e. a change in scale will change the ICER ratio even though original figures remain the same).
Benchmarking against other interventions provides valuable contextual information. The <a href="https://research.tufts-nemc.org/cear4/default.aspx">Tufts Cost-Effectiveness Analysis Registry</a> is a valuable resource in this regard.</p>

<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Modest Evaluation and the Burden of Proof]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/11/evaluation-and-the-burden-of-proof/"/>
    <updated>2012-12-11T16:04:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/11/evaluation-and-the-burden-of-proof</id>
    <content type="html"><![CDATA[<p>Impact evaluation studies are often called upon to put a price on something that is impossible to value. Evaluators ought to take this limitation seriously when designing their research methods. This post discusses the problem and suggests some responses.</p>

<!-- more -->


<p>The <strong>burden of proof</strong> is said to lie with the party making a claim. In order to convince someone of some truth, you are obliged to support your claim with evidence. The audience hearing your claim will make a judgement by scrutinising the evidence you present in support.</p>

<p>This obligation should not be treated lightly. In some cases, it isn&rsquo;t possible to marshall evidence to support our intuitions. Indeed we can&rsquo;t always know what we don&rsquo;t know and so we shouldn&rsquo;t act like we can. Recognition of this limitation is known as <strong>epistemological modesty</strong>. This perspective doesn&rsquo;t mean that we have to admit defeat. Instead it is basis for action. Action that takes place with an awareness of our necessarily limited understanding.</p>

<p>This approach has important lessons for evaluation. It suggests that we should avoid the temptation to reduce the nuances of impact evaluation to a single result (i.e. an absolute figure in pounds sterling for the overall impact or a summary cost-benefit ratio). Although it might seem appealing to be able to boast about a large absolute impact figure or to point to a high return on investment, the evidence required to support those definite figures may not be credible. We should not invite the audience to question and undermine the claims being made. Instead we ought to make modest claims that allow for the greatest degree of flexibility over the evidence. <strong>It&rsquo;s better to be confident in a modest conclusion than unconfident in a bold one</strong>. Our efforts should be focussed on validating our assumptions and understanding situations under which our conclusions might be undermined. Thus the burden of proof is lightened and the claims more credibile.</p>

<p>Our ability to do this will, of course, depend upon the circumstance but examples of how this might work in practice include:</p>

<ul>
<li>establishing the minimum level of evidence required to demonstrate that benefits are at least equal to costs (rather than straining evidence to calculate the maximum plausible ratio of benefits to costs),</li>
<li>comparing projects on the basis of the credibility of the evidence required to demonstrate that break-even position,</li>
<li>deriving relative rather than absolute measures (so that we be certain that the comparisons are fair and reasonable,)</li>
<li>using confidence-intervals rather than point-estimates to make inferences, and</li>
<li>using evaluation to guide research (to identify otherwise implicit assumptions that need to be validated).</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is Infonomics?]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/06/what-is-infonomics/"/>
    <updated>2012-12-06T12:45:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/06/what-is-infonomics</id>
    <content type="html"><![CDATA[<p>When I started my <a href="http://infonomics.ltd.uk">economic consultancy and information analysis business</a> back in 2007 I was searching for a name that would describe my work in economics and information analysis and that was unique enough to appear early on in the search enginge result pages without too much trouble. At that time is was still cool to use a portmanteau (this was before dropping out a vowel caught-on) so I was certain that I was on to a winner.</p>

<!--more-->


<p>I didn&rsquo;t realise for some time that the term had already been coined by Doug Laney, then at META Group, to describe the practice of treating information as an asset. As an intangible asset information is given a price, appears on financial accounts, and is managed accordingly.</p>

<p>Since this definition now seems to dominate the <a href="http://en.wikipedia.org/w/index.php?title=Infonomics">wikipedia page</a> on the subject, I think it&rsquo;s about time I added my voice to the debate.</p>

<p>In my mind, at least, <strong>Infonomics is about information-based on economics, not economics-based on information</strong>. Whereas econometrics refers to economic statistics (and, let&rsquo;s be honest, principally regression analysis), I imagined Infonomics as a super-set including elements of computer science (such as information theory, algorithmic design, artificial intelligence, and data visualisation).</p>

<p>Of course there&rsquo;s rarely a single, unambigious definition of a neologism. But at least we can all agree that <a href="http://en.wikipedia.org/w/index.php?title=Infonomics&amp;diff=prev&amp;oldid=471082260">wikipedia should no longer say</a> &ldquo;nobody has as yet referred to himself/herself as an infonomist&rdquo;.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[5 Questions to Consider Before Commissioning an Evaluation]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/12/05/questions-to-consider-before-commissioning-an-evaluation/"/>
    <updated>2012-12-05T10:50:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/12/05/questions-to-consider-before-commissioning-an-evaluation</id>
    <content type="html"><![CDATA[<p>Planning is fundamental to successful evaluation. Considering these questions will help you to understand what you need to discuss with a contractor so that they can design an evaluation effectively.</p>

<!-- more -->


<h2>What is the objective?</h2>

<p>This ought to be defined in terms of the key questions that the evaluation ought to answer. The choice of question will determine which methods are used and what data is gathered.
Typical objectives include wanting to:</p>

<ul>
<li>have evidence to attract funding or investment</li>
<li>identify what has worked and what hasn&rsquo;t to make improvements</li>
<li>share lessons learned with partners</li>
<li>build plan for future activity</li>
</ul>


<p>Clearly a different approach to evaluation would be required to answer each of these questions.</p>

<p>In order to define the key evaluation question(s) it might be useful to consider:</p>

<ul>
<li>Who is the audience is for the evaluation (your board of directors, external funders, or participants)?</li>
<li>What will you do with the evaluation once it&rsquo;s complete?</li>
<li>What decisions will the evaluation help you make?</li>
</ul>


<h2>Who are your stakeholders or beneficiaries?</h2>

<p>Different people will value your work for different reasons. Those reasons won&rsquo;t always overlap and may even conflict. The evaluation might reach a different conclusion based upon the perspective you take. Answering this question will help you to determine scope and focus &ndash; indeed it&rsquo;s sometimes useful to think about the maximum possible range of beneficiaries just so that you can determine those whom won&rsquo;t be considered.</p>

<p>Thinking about stakeholders will also help with the preceding question. Often the stakeholder may also be a funder. It&rsquo;s important in these cases to have the relevant people involved in the evaluation from the outset. This means discussing the objectives and proposed methods with them. Not only will this give your conclusions greater credibility with the target audience, but it will also help to ensure relevance. Further, they might be able to offer insight or resources from other work they&rsquo;ve seen.</p>

<h2>What can we do internally, where do we need support?</h2>

<p>You can expect good evaluation to cost at least 10% of the operating costs of the service being evaluated. It not necessary to outsource all of the evaluation work. Even where you are contractually obliged to appoint an independent consultant it is still possible for you to be involved.</p>

<p>External support will typically be need for technical aspects such as:</p>

<ul>
<li>establish a research methodology or epsitemological framework for answering your key evaluation questions</li>
<li>designing primary research (sampling, questionnaires, or topic guides)</li>
<li>devising and analysing statistical models to establish behavioural change, causal effects or to financial proxies for non-traded costs and benefits</li>
</ul>


<p>Whereas internal work might focus on activities like collecting monitoring data.</p>

<p>As discussed above, it is vital that internal staff are involved in the conversations with stakeholders if the evaluation is going to be useful in future.</p>

<h2>Do we understand the evaluation design?</h2>

<p>In order to interpret the results of the evaluation it will be vital for you to understand how those results were reached. This doesn&rsquo;t necessarily mean, for example, that you need to know how to calculate a &ldquo;two-stage Heckit regression&rdquo;. You do at least need to know why this method was used (i.e. in response to selection bias) so that you know that the findings do account for the fact that sample of people consulted was biased. This is important for forming your own judgements about the results and for dealing with criticisms from others.</p>

<p>A empirically flawless method is useless if none of your audience can make sense of the results. That&rsquo;s not to say that you should avoid technical research methods; indeed this question might be better phrased as: what do we need to learn to understand the evaluation design?</p>

<h2>What data do we already have?</h2>

<p>Having data available may help to save on the costs of data collection. Data can come from obvious sources such as:</p>

<ul>
<li>the monitoring data you&rsquo;re contractually obliged to collect,</li>
<li>management information you use to direct your work, or</li>
<li>feedback forms you&rsquo;ve used to ongoing service development.</li>
</ul>


<p>Data can also come from unexpected places like:</p>

<ul>
<li>administrative databases,</li>
<li>server access records or log files, and</li>
<li>email address books or messages.</li>
</ul>


<p>You should also expect to provide evaluators with:</p>

<ul>
<li>contact information for consultees &ndash; at the strategic and beneficiary level,</li>
<li>background information (particularly any work done to scope, appraise, design or commission your service), and</li>
<li>contractual and financial records.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open For Business]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/11/13/open-for-business/"/>
    <updated>2012-11-13T13:28:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/11/13/open-for-business</id>
    <content type="html"><![CDATA[<p>This post outlines some business considerations for open source. It follows <a href="http://infonomics.ltd.uk/news/../portfolio_project/14">advice Infonomics provided to the World Wildlife Fund</a> in collaboration with <a href="http://www.ribble-consultants.co.uk">Ribble Consultants</a>.</p>

<!--more-->


<h2>Open Source Definitions</h2>

<p>The term <em>open source</em> denotes software for which the original source code is made available and may be redistributed with or without modification. In practice this is typically coupled with a peer-production development model although code can be open-sourced without providing any means of re-integrated changes back into the code base.
Note that:</p>

<ul>
<li><strong>Free software is not necessarily open-source (closed-source software may be free)</strong>.  <a href="http://www.gnu.org/philosophy/selling.html">Here ‘free’ refers to the freedom (to run, modify, and redistribute) not the price</a>.</li>
<li><strong>Open-source software is not necessarily free of charge</strong>.  There are a variety of business models for generating revenue from an open-source project including: ‘freemium’ (premium features must be paid for), consulting (selling services related to the product), support (paid-for training), subscriptions (original is free, updates require a fee), proprietary add-ons (fees for implementing/ developing bespoke features), and dual licenses (whereby only those earning money from the tool must pay).</li>
<li><strong>Open-sourcing does not necessarily involve any loss of control over the quality or integrity of the product</strong> (or intellectual property); multiple versions (internal and external) may exist concurrently.  Typically derivatives and modifications are made upon ‘forked’ copies (or branches).  The lead maintainer (person managing the code) takes responsibility for the core (trunk) version.  Changes contributed by the community (in branches) may be applied (used to patch) the original (trunk) or they may be pursued independently.  Moreover forks may pursue entirely different objectives without ever being integrated (e.g. a version is developed for a different software platform or only small portions of the code taken for use elsewhere).  See below for some real-world examples.</li>
</ul>


<h2>Advantages and Disadvantages</h2>

<p>The advantages of open source include:</p>

<ul>
<li><strong>Bug/ defect correction</strong>: with more people exploring and testing the code there is a greater chance that problems will be identified and solved (improving stability and reliability);</li>
<li><strong>Improvements/ new feature</strong>: there are more people to contribute ideas and code to enhance the product and integrate it with other developments;</li>
<li><strong>Credibility</strong>: greater transparency lends credibility (‘auditability’) and the product’s sustainability depends less on the original developer.
The disadvantages include:</li>
<li><strong>Price ceiling</strong>: the original developer may not be able to charge as much for software licenses.  We might expect that the fee could be no greater than is the cost of deploying/ implementing/ maintaining the code that is provided.</li>
<li><strong>Intellectual property</strong>: other developers may be able to implement competing versions (this is typically dealt with via licensing restrictions).</li>
</ul>


<h2>Open Source Business Models</h2>

<p>The table below describes how some of the larger businesses make money while releasing open-source versions of their software.
Typically there is:</p>

<ul>
<li>a distinction between a limited-spec version which is released open-source and a fully-featured version which is sold commercially; and/ or</li>
<li>a fundamental division of functionality along the lines of free to use, pay to develop/ sell;</li>
<li>a set of complementary services that are provided at a fee (training, support, consultancy).</li>
</ul>


<table>
<thead>
<tr>
<th></th>
<th> Company          </th>
<th> Open Source                                                                              </th>
<th> Commercial                                                                 </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Adobe            </td>
<td> Flex platform for rich internet applications                                             </td>
<td> Flash Builder integrated development environment</td>
</tr>
<tr>
<td></td>
<td> Apple            </td>
<td> Darwin: core unix compatible operating system released under Apple Public Source License </td>
<td> Mac OSX, Apple TV, iOS (iPhone/ iPad/ iPod platform)</td>
</tr>
<tr>
<td></td>
<td> Canonical        </td>
<td> Ubuntu: linux operating system                                                           </td>
<td> Technical support contracts </td>
</tr>
<tr>
<td></td>
<td> Mozilla          </td>
<td> Firefox: web browser                                                                     </td>
<td> Partnership with google and others for including search engines in Firefox</td>
</tr>
<tr>
<td></td>
<td> MySQL            </td>
<td> Database: community edition                                                              </td>
<td> Enterprise/ Cluster editions, training and consultancy</td>
</tr>
<tr>
<td></td>
<td> Sun Microsystems </td>
<td> Open Office: office productivity suite                                                   </td>
<td> Star Office</td>
</tr>
</tbody>
</table>


<h2>What to consider before going Open Source</h2>

<ul>
<li><strong>Release the code</strong>: the source will be made available online (i.e. the code not necessarily the data)</li>
<li><strong>Release data</strong>: some example data may be provided</li>
<li><strong>Provide facility for commits</strong>: developers may ‘commit’ changes or ‘fork’ new versions of the software (otherwise developers are responsible for maintain their own – external – versions)</li>
<li><strong>Allow derivative works</strong>: software developed from the tool can be sold commercially (there may be dual licenses – i.e. one for users and another for re-sellers)</li>
<li><strong>Modifications must distribute source</strong>: Licensee must include the source with any modifications (i.e. cannot create a closed-source derivative)</li>
<li><strong>Derivatives subject to same license (copy-left)</strong>: require that the same rights be preserved in modified versions</li>
<li><strong>Provide user support</strong>: for example, via training courses/ email/ telephone/ issue tracking/ webchat to end-users</li>
<li><strong>Provide development support</strong>: as above but for developers</li>
<li><strong>Facility for community discussions</strong>: a popular alternative to one-to-one support, this allows users to guide one-another (typically moderated by trust users or employees)</li>
<li><strong>What aspects are free of charge?</strong></li>
<li><strong>What aspects require a fee?</strong></li>
</ul>


<h2>Comparison of Open Source Licenses</h2>

<p>The decicions you reach on the above considerations will help to determine which license is most suitable.</p>

<table>
 <tr>
  <td>
  <p align="center"><span>License</span></p>
  </td>
  <td>
  <p align="center"><span>Attribution</span></p>
  </td>
  <td>
  <p align="center"><span>Non-Commercial</span></p>
  </td>
  <td>
  <p align="center"><span>Registration</span></p>
  </td>
  <td>
  <p align="center"><span>Distribution</span></p>
  </td>
  <td>
  <p align="center"><span>Combinations</span></p>
  </td>
  <td>
  <p align="center"><span>Transformation</span></p>
  </td>
  <td>
  <p align="center"><span>Linking</span></p>
  </td>
  <td>
  <p align="center"><span>Copyleft</span></p>
  </td>
 </tr>
 <tr>
  <td>
  <p align="center"><span>&nbsp;</span></p>
  </td>
  <td>
  <p align="center"><span>&nbsp;</span></p>
  </td>
  <td>
  <p align="center"><span>&nbsp;</span></p>
  </td>
  <td>
  <p align="center"><span>&nbsp;</span></p>
  </td>
  <td>
  <p align="center"><span>Licensee must include the source with any
  modifications</span></p>
  </td>
  <td>
  <p align="center"><span>with other code</span></p>
  </td>
  <td>
  <p align="center"><span>edit/ rewrite</span></p>
  </td>
  <td>
  <p align="center"><span>with code under different license</span></p>
  </td>
  <td>
  <p align="center"><span>Derivates subject to same license</span></p>
  </td>
 </tr>
 <tr>
  <td nowrap valign="bottom">
  <p><span>All-rights-reserved</span></p>
  </td>
  <td nowrap>
  <p align="center"><span>n/a</span></p>
  </td>
  <td nowrap>
  <p align="center"><span>n/a</span></p>
  </td>
  <td nowrap>
  <p align="center"><span>n/a</span></p>
  </td>
  <td nowrap>
  <p align="center"><span>n/a</span></p>
  </td>
  <td nowrap>
  <p align="center"><span>n/a</span></p>
  </td>
  <td nowrap>
  <p align="center"><span>n/a</span></p>
  </td>
  <td nowrap>
  <p align="center"><span>n/a</span></p>
  </td>
  <td nowrap>
  <p align="center"><span>n/a</span></p>
  </td>
 </tr>
 <tr>
  <td nowrap valign="bottom">
  <p><span>Public-domain</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Allowed</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Allowed</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
 </tr>
 <tr>
  <td nowrap valign="bottom">
  <a href="http://three.org/openart/license/"><p><span>Open-art</span></p></a>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FFFFBF">
  <p align="center"><span>Share alike</span></p>
  </td>
  <td nowrap style="background: #FFFFBF">
  <p align="center"><span>Share alike</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
 </tr>
 <tr>
  <td nowrap valign="bottom">
  <a href="http://www.gnu.org/licenses/gpl.html"><p><span><acronym title="General Public License">GPL</acronym></span></p></a>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FFFFBF">
  <p align="center"><span>Share alike</span></p>
  </td>
  <td nowrap style="background: #FFFFBF">
  <p align="center"><span>Share alike</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
 </tr>
 <tr>
  <td nowrap valign="bottom">
  <a href="http://www.gnu.org/copyleft/lesser.html"><p><span><acronym title="Lesser General Public License">LGPL</acronym></span></p></a>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Allowed</span></p>
  </td>
  <td nowrap style="background: #FFFFBF">
  <p align="center"><span>Share alike</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
 </tr>
 <tr>
  <td nowrap valign="bottom">
  <a href="http://www.apache.org/licenses/LICENSE-2.0.html"><p><span>Apache 2.0</span></p></a>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Allowed</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Allowed</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
 </tr>
 <tr>
  <td nowrap valign="bottom">
  <a href="http://www.linfo.org/bsdlicense.html"><p><span><acronym title="Berkley Software Distribution">BSD</acronym></span></p></a>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Allowed</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Allowed</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
 </tr>
 <tr>
  <td nowrap valign="bottom">
  <a href="http://mit-license.org"><p><span><acronym title="Massachusetts Institute of Technology">MIT<acronym></span></p></a>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Allowed</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Allowed</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
 </tr>
 <tr>
  <td nowrap valign="bottom">
  <a href="http://creativecommons.org/licenses/by/3.0"><p><span><acronym title="Creative Commons Attribution">CC by</acronym></span></p></a>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Allowed</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Allowed</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
 </tr>
 <tr>
  <td nowrap valign="bottom">
  <a href="http://creativecommons.org/licenses/by-sa/3.0"><p><span><acronym title="Creative Commons Attribution ShareAlike">CC by-sa</acronym></span></p></a>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FFFFBF">
  <p align="center"><span>Share alike</span></p>
  </td>
  <td nowrap style="background: #FFFFBF">
  <p align="center"><span>Share alike</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
 </tr>
 <tr>
  <td nowrap valign="bottom">
  <a href="http://creativecommons.org/licenses/by-nc-nd/3.0"><p><span><acronym title="Creative Commons Attribution Non-Commerical No-Derivatives">CC by-nc-nd</acronym></span></p></a>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
  <td nowrap style="background: #91CF60">
  <p align="center"><span>Yes</span></p>
  </td>
  <td nowrap style="background: #FC8D59">
  <p align="center"><span>No</span></p>
  </td>
 </tr>
</table>


<p>The GPL and LGPL Version 3 prohibits DRM, locked-down firmware, and patent lawsuits.</p>

<p>The BSD license prohibits the use of copyright holder&rsquo;s name for promotion.</p>

<p>The Apache license requires that every file must contain all information about changes, copyrights and patent protection.</p>

<p>The <a href="http://osrc.blackducksoftware.com/data/licenses/#top20">Black Duck Knowledge Base</a> suggests that GPL2 is the most popular choice.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Economics of Open Data]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2012/10/25/the-economics-of-open-data/"/>
    <updated>2012-10-25T15:35:00+01:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2012/10/25/the-economics-of-open-data</id>
    <content type="html"><![CDATA[<p>Data doesn&rsquo;t make for a very good tradable commodity. Technological changes that allow us to connect and share data more easily are disrupting established business models. Therefore the public sector has a critical role to intervene in the data market and the private sector must adapt to ensure long-term sustainability.</p>

<!--more-->


<h2>The Market Isn&rsquo;t Always Perfect</h2>

<p>Economists use this term to describe situations where the allocation of goods and services by the free market is inefficient (i.e. someone can be made better off without making another worse off). <strong>In some extreme cases the good or service would not be available without intervention by government</strong>. The market is only effective if the good or service:</p>

<ul>
<li>has a price which reflects all costs and benefits</li>
<li>can be withheld from people at reasonable expense</li>
<li>cannot be used by more than one person at the same time</li>
</ul>


<p>These characteristics don&rsquo;t apply in the case of open data&hellip;</p>

<h2>Open Data: The Gift That Keeps On Giving</h2>

<p>An externality, or spillover effect, is a cost or benefit that is incurred by someone who is not involved in the trade. The price will not reflect these indirect social impacts. Pollution is an example of how external costs can lead to excessive production.</p>

<p><em>Network externalities</em> are a form of external benefit. Within a network the value of an adding an extra node or edge (e.g. person or friendship) is felt by every other part of the network. The Metrolink extension, for example, is not only of benefit to people in the new areas that can now use trams, but also to people in the existing areas that can now access new areas.</p>

<p>Network externalities occur in the case of open data because data is only informative when it is interpreted. The value of each datum increases with the volume of other data that it may be connected with as the context and range of analysis that are possible increases. Thus, data operates as a network. In deciding whether or not to open their data, <strong>private organisations will not consider the indirect benefits that their data will have to improve the quality of every other dataset</strong> and so will release less data than is socially optimal.</p>

<h2>Data As A Public Good</h2>

<p>A <em>public good</em> is one which is non-excludable (impossible to prevent people from using it) and non-rival (one person&rsquo;s use doesn&rsquo;t not reduce availability to another). The problem with these goods is that consumers can take advantage without contributing sufficiently to their creation. If too many people decide to <em>free-ride</em> then the private revenues won&rsquo;t meet the private costs and the incentive to provide the good through the market will disappear.</p>

<p>Data is typically non-excludable. Although legislation may prohibit the buyer from sharing data it is, in many cases, practically unenforcable (as is apparent with music sharing, for example). Data is also non-rival. Your reading of this post does not prevent another person from reading it &ndash; indeed I don&rsquo;t loose the ideas when I write them down &ndash; we can all enjoy them at the same time. <strong>Where data may be characterised as a public good, subject to the free-rider problem, there won&rsquo;t be sufficient private incentives for production and distribution</strong>.</p>

<h2>Implications For Business &amp; Government</h2>

<p>So what does this economic analysis tell us?</p>

<ol>
<li>Technological change (particularly the increasing capacity for connecting and sharing data) will disrupt certain markets by undermining established closed-data business models;</li>
<li>Businesses that attempt to defend outdated business models by keeping data closed won&rsquo;t be able to compete with an open alternatives (alternatives that they are encouraging by keeping their data closed);</li>
<li>It&rsquo;s probably better for such businesses to open their data before a competitor does &ndash; they shouldn&rsquo;t waste their current position of dominance;</li>
<li>In some cases, where the market fails completely, public intervention to fund the creation or provision of information will help to improve economic growth and social welfare;</li>
<li>In other cases there will be considerable <a href="http://infonomics.ltd.uk/news/blog/2013/03/01/open-data-business-models/">opportunities for private firms to innovate and capture the benefits of open data</a>.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Winning Public Sector Contracts]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2011/11/30/winning-public-sector-contracts/"/>
    <updated>2011-11-30T13:11:00+00:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2011/11/30/winning-public-sector-contracts</id>
    <content type="html"><![CDATA[<p>I was invited to provide some training at the <a href="http://www.socialsolutionsacademy.co.uk/"><acronym title="Social Solutions Academy">SSA</acronym></a>&rsquo;s <a href="http://www.socialsolutionsacademy.co.uk/past-events/">Social Enterprise Action Day</a>.</p>

<p>One of the participants was kind enough to say:</p>

<blockquote><p>The &#8216;Winning public contracts&#8217; session very good, I was very impressed with content. The facilitator had undertaken some research about my social enterprise prior to the event and found a relevant funding opportunity. This was used as a case study during the session.</p><footer><strong>Rose Marley, Director</strong> <cite><a href='http://www.motiv.org.uk/'>Motiv CIC</a></cite></footer></blockquote>




<iframe src="http://www.slideshare.net/slideshow/embed_code/15208098" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="http://www.slideshare.net/robingower/winning-public-sector-contracts" title="Winning Public Sector Contracts" target="_blank">Winning Public Sector Contracts</a> </strong> from <strong><a href="http://www.slideshare.net/robingower" target="_blank">robingower</a></strong> </div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An Introduction to GNU-R]]></title>
    <link href="http://infonomics.ltd.uk/news/blog/2010/09/23/invitation-to-an-introduction-to-gnu-r-talk/"/>
    <updated>2010-09-23T00:00:00+01:00</updated>
    <id>http://infonomics.ltd.uk/news/blog/2010/09/23/invitation-to-an-introduction-to-gnu-r-talk</id>
    <content type="html"><![CDATA[<p>The Manchester Free Software group has invited me to give a talk on&nbsp;<a target="_blank" href="http://www.r-project.org/">GNU-R</a>.</p>


<p><a target="_blank" href="http://libreplanet.org/wiki/Manchester/2010-10-19">http://libreplanet.org/wiki/<wbr></wbr>Manchester/2010-10-19</a></p>


<ul>
<li><a title="http://madlab.org.uk/" rel="nofollow" target="_blank" href="http://madlab.org.uk/">Madlab</a>&nbsp;(Manchester Digital Laboratory) Upstairs,&nbsp;36-40&nbsp;<a title="http://www.openstreetmap.org/?lat=53.484241&amp;lon=-2.236324&amp;zoom=18&amp;layers=B000FTF" rel="nofollow" target="_blank" href="http://www.openstreetmap.org/?lat=53.484241&amp;lon=-2.236324&amp;zoom=18&amp;layers=B000FTF">Edge Street</a>, Manchester, M4 1HN (between Thomas St and the Craft Centre, opposite A Bar Called Common).</li>
<li>Tuesday, 19th October 2010 (3rd Tuesday of the month) 19:00 &ndash; 20:30</li>
</ul>


<p>R is a system for statistical computation and graphics. &nbsp;At Infonomics we use R to handle much of our analytic work, particularly where there is repeated processing of data, complex statistical models, or requirements for data graphics that go beyond the limitations of spreadsheets. &nbsp;We&#8217;ve used it, for example, in the&nbsp;<a href="http://reports.infonomics.ltd.uk/">AutoReporter</a>.</p>


<p>The talk will provide: an introduction to R, discussion of when (and when not) to use R, and a guided tour of it&#8217;s capabilities with examples. &nbsp;It&#8217;s also a chance to learn about the secrets of some of our tools of the trade. We expect that audience will comprise those with a interest in data/ statistics, software and/ or visualisation.</p>


<p><a href="http://libreplanet.org/wiki/Manchester">Manchester Free Software</a>&nbsp;was formed in response to the growing need for a group based in the Manchester area that focuses on Free Software and GNU/Linux primarily, but also on issues such as Digital Restrictions Management and other issues which infringe on the freedoms of computer users.</p>

]]></content>
  </entry>
  
</feed>
